1. Introduction and Project GoalsThis document outlines the technical specifications for a Graph-Powered Hybrid Retrieval Augmented Generation (Graph RAG) system. The primary objective is to develop a robust and scalable solution capable of ingesting diverse document sources, constructing a dynamic knowledge graph, and enabling sophisticated querying through a combination of graph-based and vector-based retrieval mechanisms. The system will leverage the graphitti framework for knowledge graph creation and management within a Neo4j AuraDB instance. Retrieval and generation will be orchestrated by LlamaIndex, utilizing Google's latest Gemini models via Vertex AI. A Supabase vector store will serve as a complementary retrieval source for unstructured text. The user interface will be a Streamlit-based chat application.A key aspect of this project is the investigation into the feasibility and potential benefits of incorporating Agentic RAG principles to enhance the system's reasoning and retrieval capabilities. This document will detail the architecture, component specifications, data flows, integration points, and deployment considerations, providing a comprehensive technical blueprint for development.Core Project Goals:
Develop a data ingestion pipeline using graphitti to process documents and build a temporally-aware knowledge graph in Neo4j AuraDB.
Implement a hybrid retrieval system combining structured graph queries (keyword, semantic, path-based) from Neo4j with semantic vector search from Supabase.
Integrate Google Gemini models (via Vertex AI) for natural language understanding, generation, and embedding tasks, orchestrated by LlamaIndex.
Provide a Streamlit-based chat interface for user interaction, potentially complemented by advanced graph visualization tools like Neo4j Bloom.
Evaluate the potential of Agentic RAG to improve query understanding, retrieval strategy, and overall system performance.
Ensure robust integration with specified Google Cloud Platform (GCP) services.
2. System Architecture OverviewThe proposed system architecture is designed around a modular set of components facilitating data ingestion, knowledge graph construction, hybrid retrieval, language model interaction, and user interfacing.Key Components:
Data Ingestion & Knowledge Graph Creation (graphitti & Neo4j AuraDB): Documents are processed by graphitti, which extracts entities and relationships, populating a Neo4j AuraDB knowledge graph. This component is responsible for maintaining the temporal accuracy and dynamic nature of the graph.
Vector Store (Supabase): Alongside the knowledge graph, relevant text chunks or documents will be embedded and stored in a Supabase PostgreSQL database utilizing the pgvector extension for efficient similarity search.
Orchestration & Retrieval (LlamaIndex): LlamaIndex will serve as the central framework for managing data, orchestrating retrieval from both Neo4j and Supabase, and interacting with the LLM.
Language Model Services (Google Gemini via Vertex AI): Google's Gemini models, accessed through Vertex AI, will provide the core AI capabilities for embedding generation, natural language understanding, and response generation.
User Interface (Streamlit): A Streamlit application will provide a chat-based interface for users to interact with the RAG system.
(Optional) Agentic Layer (Google ADK or LlamaIndex Agents): If implemented, an agentic layer would sit atop the retrieval and LLM components to enable more complex reasoning and tool use.
Diagram 1: High-Level System ArchitectureCode snippetgraph TD
    A --> B{graphitti Engine};
    B -- Entities & Relationships --> C;
    A -- Text Chunks --> D{Embedding Model via Vertex AI};
    D -- Embeddings --> E;
    F[User] --> G;
    G -- Query --> H{LlamaIndex Orchestrator};
    H -- Graph Query --> C;
    H -- Vector Query --> E;
    C -- Retrieved Graph Context --> H;
    E -- Retrieved Vector Context --> H;
    H -- Prompt + Context --> I{Gemini LLM via Vertex AI};
    I -- Generated Response --> H;
    H -- Final Answer --> G;
This architecture supports a hybrid retrieval strategy where context can be pulled from the structured knowledge graph (offering precision and relationship-based insights) and the vector store (offering broad semantic similarity). The LlamaIndex orchestrator is pivotal in managing these heterogeneous data sources and synthesizing them for the LLM.3. Data Ingestion and Knowledge Graph Creation with graphittiThe foundation of the Graph RAG system is the dynamic knowledge graph (KG) built and maintained by graphitti. graphitti is specifically designed for AI agents operating in dynamic environments, offering capabilities beyond traditional RAG by continuously integrating data into a coherent, queryable graph.13.1. graphitti-core Overviewgraphitti-core is a Python framework for building and querying temporally-aware knowledge graphs.1 Its core strength lies in its ability to autonomously construct a KG while managing changing relationships and preserving historical context.1 This is achieved through real-time incremental updates, eliminating the need for batch recomputation when new data arrives.1 A bi-temporal data model tracks both the event occurrence time and ingestion time, enabling precise point-in-time queries and analysis of data evolution.1Key features relevant to this project include:
Real-Time Incremental Updates: New data episodes are immediately integrated without requiring full graph recomputation.1
Bi-Temporal Data Model: Tracks event occurrence and ingestion times for historical accuracy.1
Efficient Hybrid Retrieval: Combines semantic embeddings, keyword (BM25) search, and graph traversal for low-latency queries, often without LLM summarization during retrieval.1
Custom Entity Definitions: Supports flexible ontology creation using Pydantic models to define domain-specific entities, enhancing the precision of context extraction.1 This allows for a more guided and structured information extraction process by the LLM, moving beyond generic entity recognition.
LLM Integration: graphitti leverages LLMs for inference and embedding. It performs optimally with LLMs that support structured output, such as Google Gemini, as this ensures correct schema mapping and reduces ingestion failures.1 The quality of the graph built by graphitti is therefore directly influenced by the structured output capabilities of the chosen Gemini model.
3.2. Neo4j AuraDB ConfigurationThe system will use a Neo4j AuraDB instance as the backend for graphitti. Connection details are provided in the .env file (Image 1):
NEO4J_URI: neo4j+s://037bf0a4.databases.neo4j.io
NEO4J_USER: wayne.painters@outlook.com (Note: For production, using a dedicated service account instead of a personal email is highly recommended for security and manageability.)
NEO4J_PASSWORD: (As provided in .env)
NEO4J_DATABASE: neo4j
AURA_INSTANCEID: 037bf0a4
AURA_INSTANCENAME: Instance01
graphitti will manage the schema, indices, and constraints within this Neo4j database.13.3. graphitti Schema: Nodes, Edges, and PropertiesWhile graphitti can autonomously build a KG, its schema is influenced by the input data and any custom entity definitions provided.3 Based on graphitti's design:
Nodes (Entities): Represent extracted entities from documents (e.g., persons, organizations, locations, concepts). Nodes will have labels (e.g., Entity, or custom types defined via Pydantic models like Person, Company).

Properties: id (unique identifier), name (entity name), description (LLM-generated summary if applicable), embedding (vector embedding for semantic search), temporal properties managed by graphitti.


Edges (Relationships): Represent connections between entities (e.g., "works_at", "located_in", "mentions"). Edges will have types (e.g., RELATES_TO, or more specific types derived by the LLM).

Properties: source_node_id, target_node_id, description (context of the relationship), t_valid (when the relationship became valid), t_invalid (when it became invalid), t_ingested (when it was added to the graph).1


Episode Nodes: graphitti processes data as "episodes".1 These episodes (e.g., a document, a user interaction) are linked to the entities and relationships extracted from them, providing provenance.
Text Chunks/Source Nodes: The raw text from which information is extracted might be stored or referenced, linking back to the entities and relationships to provide grounding for RAG.
graphitti's bi-temporal model is a significant differentiator. It explicitly tracks when an event occurred in the real world and when it was ingested into the graph. Each relationship (edge) includes validity intervals (t_valid, t_invalid), allowing the system to preserve historical accuracy without discarding outdated information, which is crucial for dynamic datasets.13.4. Document Processing Pipeline with graphitti
Input: Documents from various sources (e.g., text files, structured JSON from Google Drive as indicated by DRIVE_FOLDER_ID in Image 1).
Preprocessing: Standard text cleaning, potentially document parsing.
Episode Creation: Input data is treated as an "episode" by graphitti.1
Entity & Relationship Extraction: graphitti uses the configured LLM (Gemini) to extract entities and their relationships from the episode content. If custom Pydantic entity types are defined, the LLM will be guided to extract these specific structures.3
Embedding Generation: The configured embedder (Gemini) generates embeddings for entities and potentially relationships or text chunks for semantic search capabilities within graphitti.
Graph Update: Extracted entities and relationships are added to the Neo4j graph. graphitti handles de-duplication of nodes and manages the temporal lifecycle of relationships.1
Indexing: graphitti ensures necessary indices (vector and BM25 keyword) are created in Neo4j for efficient retrieval.1
3.5. Key Code Examples: graphitti Initialization and Data Ingestion

Initializing graphitti with Google Gemini (via Vertex AI context):The .env file (Image 1) specifies Gemini models and a Google Cloud Project. graphitti-core supports Google Gemini through the graphiti-core[google-genai] extra.1
Python# Based on [1] and.env (Image 1)
from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig
from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig

# From.env (Image 1)
NEO4J_URI = "neo4j+s://037bf0a4.databases.neo4j.io"
NEO4J_USER = "wayne.painters@outlook.com"
NEO4J_PASSWORD = "o1ocsUT2c2Ye-382nkMHwppYeK6zY3BIxVm2LwZgk" # Actual password from.env
# For Gemini client, GOOGLE_API_KEY is specified in.env.
# If running in a GCP environment with Application Default Credentials (ADC) configured,
# the API key might not be explicitly needed for Vertex AI.
# However, graphiti's GeminiClient might expect it.
GOOGLE_API_KEY = "AIzaSyChzJNQIp6p2OIoEXzFqEbXYuQThwdOgNUK" # From.env (Image 1/2)

# Model identifiers from.env (Image 1)
# For graphiti's LLM client (extraction, reasoning)
LLM_MODEL_FOR_GRAPHITTI = "gemini-2.5-pro"
# For graphiti's embedder
# Graphiti's GeminiEmbedder expects model names like "embedding-001" or similar. [1]
# The.env variable EMBED_MODEL_LLM="gemini-2.5-flash" is a generative model.
# This needs careful consideration: either GeminiEmbedder can use a generative model for embeddings,
# or a specific embedding model ID compatible with GeminiEmbedder needs to be used.
# For Vertex AI, embedding models are typically "text-embedding-004", etc.
# The example in [1] uses "embedding-001" for GeminiEmbedder.

EMBED_MODEL_FOR_GRAPHITTI = "embedding-001" # As per graphiti's GeminiEmbedder example [1]
                                           # Or if gemini-2.5-flash can be used: "gemini-2.5-flash"

GCP_PROJECT_ID_FOR_MODELS = "neo4j-deployment-new1" # From.env (Image 1)
GCP_LOCATION_FOR_MODELS = "us-central1" # From.env (Image 1)

# It's important to verify how graphiti's GeminiClient and GeminiEmbedder
# handle Vertex AI specific model paths (e.g., "projects/.../locations/.../publishers/google/models/gemini-2.5-pro")
# versus more generic model names. The example in [1] uses simpler model names.

graphiti_instance = Graphiti(
    NEO4J_URI,
    NEO4J_USER,
    NEO4J_PASSWORD,
    llm_client=GeminiClient(
        config=LLMConfig(
            api_key=GOOGLE_API_KEY,
            model=LLM_MODEL_FOR_GRAPHITTI
            # Add project_id, location if GeminiClient supports Vertex AI context
        )
    ),
    embedder=GeminiEmbedder(
        config=GeminiEmbedderConfig(
            api_key=GOOGLE_API_KEY,
            embedding_model=EMBED_MODEL_FOR_GRAPHITTI # This model ID needs to be compatible
            # Add project_id, location if GeminiEmbedder supports Vertex AI context
        )
    )
)

# Initialize indices and constraints required by graphiti
# This step is crucial for graphiti to function correctly. [1]
# graphiti_instance.create_indices_constraints() # The exact method name should be verified from graphiti documentation.
print("Graphiti initialized with Gemini clients.")



Adding text episodes to the graph:
Python# Conceptual example, actual API might vary slightly based on graphiti version
# episode_content = "Kendra loves Adidas shoes. She bought them yesterday during the summer sale."
# metadata = {"source_document_id": "doc123", "category": "product review"}
# graphiti_instance.add_episode(
#     text=episode_content,
#     source_description="User chat log from retail_app", # Helps in organizing data if multiple sources
#     group_id="retail_interactions", # For namespacing data within Neo4j [6]
#     metadata=metadata
# )
# print(f"Added text episode: {episode_content}")



Adding structured JSON episodes:graphitti can also ingest structured JSON data, which is useful for integrating data from other systems or pre-processed information.1
Python# structured_data = {
#     "user_id": "U123",
#     "action": "purchased",
#     "item_details": {"name": "Adidas Gazelle", "category": "footwear", "price": 85.00},
#     "timestamp": "2023-10-26T10:00:00Z",
#     "store_location": "Online"
# }
# metadata = {"event_source": "order_processing_system"}
# graphiti_instance.add_episode(
#     json_data=structured_data,
#     source_description="Order system event for U123",
#     group_id="retail_transactions", # For namespacing data within Neo4j [6]
#     metadata=metadata
# )
# print(f"Added structured JSON episode for user {structured_data['user_id']}")


The dynamic, incremental updates and bi-temporal model of graphitti are central to its utility.1 This ensures the KG can reflect evolving information without costly rebuilds, a critical feature for systems dealing with frequently changing data sources. The ability to define custom entity types via Pydantic models provides a powerful mechanism to tailor the knowledge extraction process to specific domain requirements, yielding a more semantically rich and accurate KG.14. Hybrid Retrieval Strategy with LlamaIndexThe system will employ a hybrid retrieval strategy, orchestrated by LlamaIndex, drawing information from both the Neo4j knowledge graph (populated by graphitti) and a Supabase vector store. This approach aims to combine the strengths of structured, contextual graph queries with broad semantic vector search.4.1. Neo4j Graph RetrievalLlamaIndex provides robust integrations for Neo4j, allowing for diverse query mechanisms against the graph.7
Keyword Search: Leveraging Neo4j's native full-text indexing capabilities and graphitti's use of BM25 indexing for efficient keyword searches on node and edge properties.1
Vector Semantic Search: graphitti stores embeddings for entities (and potentially relationships) within Neo4j. LlamaIndex can perform semantic similarity searches against these embeddings.1
Graph Traversal (Cypher Queries): For complex queries involving paths, subgraphs, or specific relationship patterns, direct Cypher queries are essential. LlamaIndex's TextToCypherRetriever can translate natural language questions into Cypher queries.7 The effectiveness of this translation is highly dependent on the LLM's understanding of the graph schema and its ability to generate correct Cypher. To mitigate potential inaccuracies, LlamaIndex Workflows can be employed to implement multi-step approaches involving retries or self-correction mechanisms for the generated Cypher queries, as this technology is still evolving.8
graphitti-Specific Retrieval: graphitti itself offers predefined search recipes and a hybrid search mechanism for relationships/edges, which can be exposed and utilized through LlamaIndex if custom tools are built.1
4.2. Supabase Vector Store RetrievalA Supabase PostgreSQL instance, configured with the pgvector extension, will serve as the vector store for RAG embeddings from general document content.
Configuration: LlamaIndex's SupabaseVectorStore will be used.9 Connection parameters will be derived from the .env file (Image 1):

SUPABASE_URL: https://odpykcmtcwmyfolsrgcu.supabase.co
SUPABASE_KEY: (Service Role Key from .env)
SUPABASE_PROJECT_ID: odpykcmtcwmyfolsrgcu (derived from URL)
SUPABASE_DATABASE_PASSWORD: Hlabaysirdtgycu
SUPABASE_DATABASE_NAME: postgres
The postgres_connection_string for LlamaIndex will be constructed as: postgresql://postgres:{SUPABASE_DATABASE_PASSWORD}@db.{SUPABASE_PROJECT_ID}.supabase.co:5432/{SUPABASE_DATABASE_NAME}.


Embedding Model: The EMBED_MODEL_LLM variable from .env (Image 1: models/gemini-2.5-flash) will be used via Vertex AI, integrated with LlamaIndex. The choice of embedding model impacts both graphitti's internal semantic search and the Supabase vector store. Consistency in embedding models or a clear strategy for handling potentially different embedding spaces is important to ensure coherent semantic retrieval across both stores.
Metadata Filtering: Supabase and pgvector, when used with LlamaIndex, support metadata filtering, allowing vector searches to be refined based on specific document attributes.7
4.3. Fusion and Re-ranking of Heterogeneous ResultsCombining results from Neo4j (which may include structured data, entity summaries, relationship details, and text chunks linked to graph elements) and Supabase (primarily text chunks with vector similarity scores) is a critical step.
Strategies:

LlamaIndex's RouterRetriever can be configured to direct queries to the appropriate store (Neo4j or Supabase) or to query both and merge results.9
Custom LlamaIndex query pipelines can be designed to implement more sophisticated fusion logic.


Re-ranking:

Standard re-ranking algorithms like Reciprocal Rank Fusion (RRF) can be applied.
graphitti offers a graph distance-based re-ranking mechanism, which could be particularly useful if a central topic or entity from the query can be identified in the graph.1 This method prioritizes results that are closely connected to the query's core subject within the graph structure.


The hybrid retrieval approach is fundamental. Neo4j, powered by graphitti's model, provides access to structured, contextual, and temporally nuanced information. Supabase complements this with efficient semantic search over potentially larger or different corpora of text. The main challenge and opportunity lie in the intelligent fusion of these diverse result sets to provide the most comprehensive context to the LLM.4.4. Key Code Examples: LlamaIndex Retriever Configurations

Initializing SupabaseVectorStore and VectorStoreIndex:
Python# Based on [10] and.env (Image 1)
from llama_index.vector_stores.supabase import SupabaseVectorStore
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.embeddings.vertex import VertexAIEmbedding
from llama_index.core.settings import Settings # Assuming Settings.embed_model is configured

# Construct Supabase connection string from.env variables
PROJECT_ID_SUPA = "odpykcmtcwmyfolsrgcu" # From SUPABASE_URL in.env
DB_PASSWORD_SUPA = "Hlabaysirdtgycu" # From SUPABASE_DATABASE_PASSWORD in.env
DB_NAME_SUPA = "postgres" # From SUPABASE_DATABASE_NAME in.env
# Default user for Supabase is 'postgres'

postgres_connection_string = f"postgresql://postgres:{DB_PASSWORD_SUPA}@db.{PROJECT_ID_SUPA}.supabase.co:5432/{DB_NAME_SUPA}"
# As per user query context, a collection for RAG embeddings exists.
# We will assume its name is 'rag_documents' or similar based on typical usage.
# The user mentioned "RAG embeddings relating to normal RAG vector embeddings".
collection_name = "rag_document_embeddings"

# Ensure Settings.embed_model is configured, e.g., with VertexAIEmbedding
# (see section 5.4 LLM Integration for VertexAIEmbedding setup)
# Example:
# from llama_index.embeddings.vertex import VertexAIEmbedding
# Settings.embed_model = VertexAIEmbedding(model_name="text-embedding-004", project="your-gcp-project", location="us-central1")

# Dimension depends on the embedding model, e.g., Gemini text-embedding-004 is 768.
# This should match the dimension of embeddings already in the Supabase table.
# If the table is new, this dimension will be used for table creation.
embedding_dimension = 768 # Example for text-embedding-004

try:
    vector_store_supabase = SupabaseVectorStore(
        postgres_connection_string=postgres_connection_string,
        collection_name=collection_name,
        dimension=embedding_dimension
    )
    # To create an index from new documents:
    # storage_context_supabase = StorageContext.from_defaults(vector_store=vector_store_supabase)
    # index_supabase = VectorStoreIndex.from_documents(
    #     documents, # Your list of LlamaIndex Document objects
    #     storage_context=storage_context_supabase,
    #     embed_model=Settings.embed_model # Ensure this is set
    # )
    # To load an existing index:
    # index_supabase = VectorStoreIndex.from_vector_store(
    #     vector_store=vector_store_supabase,
    #     embed_model=Settings.embed_model # Ensure this is set
    # )
    print(f"SupabaseVectorStore for collection '{collection_name}' initialized conceptually.")
except Exception as e:
    print(f"Error initializing SupabaseVectorStore: {e}")



Initializing Neo4j graph store and relevant retrievers:
Python# Based on [7] and.env (Image 1)
from llama_index.core import PropertyGraphIndex
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.retrievers.neo4jgraph import Neo4jGraphRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.llms.vertex import VertexAI
from llama_index.core.settings import Settings

# Neo4j connection details from.env (Image 1)
NEO4J_URI_ENV = "neo4j+s://037bf0a4.databases.neo4j.io"
NEO4J_USER_ENV = "wayne.painters@outlook.com"
NEO4J_PASSWORD_ENV = "o1ocsUT2c2Ye-382nkMHwppYeK6zY3BIxVm2LwZgk"
NEO4J_DATABASE_ENV = "neo4j"

# LLM for Text2Cypher (ensure Settings.llm is configured, see section 5.4)
# Example:
# Settings.llm = VertexAI(model_name="gemini-2.5-pro", project="your-gcp-project", location="us-central1")

try:
    graph_store_neo4j = Neo4jPropertyGraphStore(
        username=NEO4J_USER_ENV,
        password=NEO4J_PASSWORD_ENV,
        url=NEO4J_URI_ENV,
        database=NEO4J_DATABASE_ENV,
    )
    print("Neo4jPropertyGraphStore initialized.")

    # Assuming the graph is already populated by graphitti.
    # For retrieval, we might use a generic Neo4jGraphRetriever or build upon PropertyGraphIndex.

    # Option 1: Generic Neo4jGraphRetriever (good for Text2Cypher and keyword/vector if schema is known)
    # This retriever can use the LLM to generate Cypher queries. [7, 8]
    # neo4j_retriever = Neo4jGraphRetriever(
    #     graph_store_neo4j,
    #     llm=Settings.llm, # For Text2Cypher
    #     # embed_model=Settings.embed_model, # For vector search if supported directly
    #     verbose=True
    # )

    # Option 2: Using PropertyGraphIndex (if we want to leverage its structured retrievers)
    # This usually assumes the index was also built via LlamaIndex. [7]
    # If graphitti builds the graph, we might need a custom retriever or adapt.
    # For conceptual purposes, if graphitti creates compatible structures:
    # property_graph_index_neo4j = PropertyGraphIndex(
    #     nodes=, # Not building, just interfacing
    #     relationships=,
    #     graph_store=graph_store_neo4j,
    #     # llm=Settings.llm, # For KG construction, not strictly needed for retrieval if graph exists
    #     # embed_model=Settings.embed_model
    # )
    # vector_retriever_neo4j = property_graph_index_neo4j.as_retriever(similarity_top_k=3, retriever_mode="vector")
    # keyword_retriever_neo4j = property_graph_index_neo4j.as_retriever(similarity_top_k=3, retriever_mode="keyword")
    # text2cypher_retriever_neo4j = property_graph_index_neo4j.as_retriever(retriever_mode="text_to_cypher")

    print("Neo4j retrievers (conceptual) initialized.")
except Exception as e:
    print(f"Error initializing Neo4j components: {e}")



5. LLM Integration: Google Gemini via Vertex AIThe system will use Google's Gemini models, accessed through Vertex AI, for all core LLM tasks, including natural language understanding, response generation, and embedding creation. LlamaIndex will facilitate the integration with these models.5.1. Model Selection and ConfigurationThe .env file (Image 1) specifies the Gemini models to be used:
Primary LLM for Generation: LLM_MODEL="gemini-2.5-pro" (from .env). This model will be used for synthesizing answers from retrieved context.
Embedding Model: EMBED_MODEL_LLM="models/gemini-2.5-flash" (from .env). This model is designated for creating embeddings.
Fallback LLM: FALLBACK_LLM_MODEL="gemini-2.5-flash" (from .env). This model can be used if the primary generation model is unavailable or for less critical generation tasks to optimize cost/latency.
The selection of a "flash" model for embeddings and fallback generation is a common practice, aiming to balance performance with cost-efficiency. More powerful "pro" models are reserved for the final answer synthesis where quality is paramount.Vertex AI Configuration:
GCP_PROJECT_ID: The .env in Image 1 shows GCP_PROJECT_ID="neo4j-deployment-new1", while Image 2 shows GOOGLE_CLOUD_PROJECT="neo4j-genai-test-project". This PRD will assume neo4j-deployment-new1 as the primary GCP project for Vertex AI services, but this needs confirmation.
GCP_LOCATION: us-central1 (from Image 1).
Table 1: Google Gemini Model Configurations and Usage
PurposeModel Name (from.env)Vertex AI IdentifierKey Configuration Parameters Main GenerationLLM_MODELgemini-2.5-protemperature: 0.28, max_output_tokens: 1024 (adjust as needed)EmbeddingEMBED_MODEL_LLMtext-embedding-004 (or compatible Gemini Flash endpoint)N/A (uses specific embedding API)Fallback GenerationFALLBACK_LLM_MODELgemini-2.5-flashtemperature: 0.3, max_output_tokens: 512 (adjust as needed)graphitti Internal LLMLLM_MODEL (or specific)gemini-2.5-pro(Depends on graphitti's internal needs)graphitti EmbedderEMBED_MODEL_LLMembedding-001 (as per graphiti example 1) or compatible Gemini Flash endpointN/A
A potential discrepancy exists: EMBED_MODEL_LLM is set to models/gemini-2.5-flash, which is a generative model identifier. Vertex AI typically uses specific embedding model IDs like text-embedding-004 or multimodalembedding@001. graphitti-core's GeminiEmbedder 1 uses embedding-001 in its example. LlamaIndex's VertexAIEmbedding class expects specific embedding model names (e.g., text-embedding-004).12 This needs to be resolved to ensure consistent and correct embedding generation across graphitti and LlamaIndex. If gemini-2.5-flash cannot directly serve as an embedding model ID for VertexAIEmbedding, then a model like text-embedding-004 should be used for LlamaIndex, and graphitti's configuration must be aligned or verified.5.2. LlamaIndex Integration with Vertex AI for GeminiLlamaIndex will connect to Gemini models on Vertex AI using the llama-index-llms-vertex and llama-index-embeddings-vertex integrations.12 Alternatively, for more structured agentic workflows, vertexai.preview.reasoning_engines.LlamaIndexQueryPipelineAgent could be considered if building Vertex AI native agents.11Authentication will primarily rely on Application Default Credentials (ADC) when the application is running within a GCP environment (e.g., Cloud Run, GKE, GCE). The GOOGLE_API_KEY provided in the .env (Image 1, Image 2) might be used for local development or if ADC is not set up. The IMPERSONATED_USER_EMAIL (kevin@clearspringcg.com from Image 1, Image 2) suggests that service account impersonation might be used, allowing the application to run with the permissions of this specified service account.5.3. Prompt Engineering for Graph RAG ContextEffective prompt engineering is crucial for guiding the Gemini LLM to synthesize information accurately from the diverse contexts retrieved from Neo4j and Supabase. Prompts should be designed to:
Clearly present the retrieved context, distinguishing between graph-derived information (entities, relationships, summaries) and text chunks from the vector store.
Instruct the LLM to synthesize these varied pieces of information into a coherent answer.
Encourage the LLM to respect temporal aspects if the query implies or graphitti provides relevant temporal metadata.
Request citations or references back to the source data segments to enhance transparency and verifiability.
5.4. Key Code Examples: LLM Initialization and Invocation within LlamaIndex

Initializing VertexAI LLM and VertexAIEmbedding in LlamaIndex:
Python# Based on [12] and.env (Image 1)
from llama_index.llms.vertex import VertexAI
from llama_index.embeddings.vertex import VertexAIEmbedding
from llama_index.core.settings import Settings

# Configuration from.env
# Ensure clarity on which GCP_PROJECT_ID to use if multiple are present in.env files
GCP_PROJECT_ID_ENV = "neo4j-deployment-new1" # As per Image 1, or "neo4j-genai-test-project" from Image 2
GCP_LOCATION_ENV = "us-central1"
LLM_MODEL_ENV = "gemini-2.5-pro" # Primary generation model

# For embeddings, using a standard Vertex AI embedding model ID.
# The.env variable EMBED_MODEL_LLM="models/gemini-2.5-flash" needs reconciliation.
# If "gemini-2.5-flash" can be used as an embedding model identifier directly, use that.
# Otherwise, a dedicated embedding model like "text-embedding-004" is standard.
EMBEDDING_MODEL_ID_VERTEX = "text-embedding-004" # Standard Vertex AI embedding model ID.
                                                # This assumes EMBED_MODEL_LLM from.env is for graphitti's specific embedder.

# Model keyword arguments for generation [11]
llm_model_kwargs = {
    "temperature": 0.28,
    "max_output_tokens": 1024, # Renamed from max_tokens for VertexAI class
    # "context_window": 200000, # Handled by model choice
}

try:
    # Set globally for LlamaIndex components
    Settings.llm = VertexAI(
        model=LLM_MODEL_ENV, # 'model_name' in older LlamaIndex versions, 'model' in newer
        project=GCP_PROJECT_ID_ENV,
        location=GCP_LOCATION_ENV,
        additional_kwargs=llm_model_kwargs # 'model_kwargs' in older versions
    )

    Settings.embed_model = VertexAIEmbedding(
        model_name=EMBEDDING_MODEL_ID_VERTEX,
        project=GCP_PROJECT_ID_ENV,
        location=GCP_LOCATION_ENV,
        # For embeddings, batch size can be specified, e.g., embed_batch_size=10
    )
    print(f"VertexAI LLM ({LLM_MODEL_ENV}) and Embedder ({EMBEDDING_MODEL_ID_VERTEX}) configured globally for LlamaIndex.")

    # Example of direct invocation (though typically done via QueryEngine)
    # response = Settings.llm.complete("What is the current status of Project Alpha based on recent reports?")
    # print(response.text)
except Exception as e:
    print(f"Error configuring VertexAI models for LlamaIndex: {e}")




Passing retrieved context to the LLM via a LlamaIndex query engine:This is typically handled internally by LlamaIndex's query engines (e.g., RetrieverQueryEngine). The engine takes the retrieved NodeWithScore objects, formats them into a context string (respecting context window limits), and prepends this context to the user query within a larger prompt template before sending it to the LLM.
Python# Conceptual example of how a LlamaIndex query engine uses the LLM
# from llama_index.core.query_engine import RetrieverQueryEngine
# from llama_index.core.response_synthesizers import get_response_synthesizer

# Assuming 'retriever' is a configured LlamaIndex retriever (hybrid, graph, or vector)
# response_synthesizer = get_response_synthesizer(llm=Settings.llm)
# query_engine = RetrieverQueryEngine(
#     retriever=retriever,
#     response_synthesizer=response_synthesizer
# )

# user_query = "What were the key findings in the latest financial report?"
# response_object = query_engine.query(user_query)
# print(f"Answer: {response_object.response}")
# for source_node in response_object.source_nodes:
#     print(f"Source Node ID: {source_node.node_id}, Score: {source_node.score}")
#     print(f"Source Text: {source_node.text[:200]}...") # Display snippet of source


6. Agentic RAG: Feasibility and Implementation PathwaysAgentic RAG involves using AI agents to enhance the Retrieval Augmented Generation process, enabling more complex reasoning, dynamic tool use, and multi-step interactions to address user queries.136.1. Assessing the Value of Agentic RAG for This ProjectAgentic RAG offers several potential benefits:
Flexibility: Agents can dynamically choose to query multiple data sources (like Neo4j and Supabase) or use various tools based on the query's nature.13
Adaptability: Instead of static rule-based retrieval, agents can adapt their strategy, potentially performing multi-hop queries in the graph or reformulating queries if initial results are insufficient.13
Accuracy: Agents could potentially validate information or cross-reference findings from different sources, leading to more accurate and reliable answers.13
However, these advantages come with trade-offs:
Increased Complexity: Designing, implementing, and orchestrating multiple agents adds significant architectural and developmental complexity.
Higher Cost and Latency: Agentic systems often involve more LLM calls for planning, tool selection, and reasoning by the agent itself, which can increase token consumption and overall response time.13
For this project, an agentic approach could be valuable if queries are often complex, ambiguous, or require synthesizing information from disparate parts of the knowledge graph and vector store in non-obvious ways. For instance, an agent could decide whether a query is best answered by historical graph data, current vector embeddings, or a combination, a task that might otherwise require intricate hard-coded logic.6.2. Potential Agent Design and RolesIf an agentic approach is adopted, several specialized agents could collaborate:
Query Planner Agent: Analyzes the incoming user query, breaks it down if necessary, and determines the optimal retrieval strategy (e.g., graph-first, vector-first, parallel hybrid, specific graph traversal path, use of graphitti's temporal query features).
Graph Query Agent: Specializes in interacting with the Neo4j knowledge graph. This agent could formulate Cypher queries (potentially using TextToCypher capabilities) or execute predefined graph algorithms. It would be responsible for leveraging graphitti's specific data model, including temporal aspects.
Vector Store Agent: Focuses on querying the Supabase vector store, handling embedding generation for the query, and applying metadata filters.
Synthesizer Agent: Receives context from the Graph Query Agent and Vector Store Agent, then prompts the main Gemini generation model to produce a final, coherent answer, ensuring all relevant information is integrated and cited.
(Optional) Validation Agent: Could be tasked with checking the consistency of retrieved information or using external tools/APIs to verify facts before final synthesis.
6.3. Implementation Approaches: Google ADK vs. LlamaIndex AgentsTwo primary frameworks are considered for implementing an agentic layer:

Google Agent Development Kit (ADK):

An open-source Python toolkit designed for building, evaluating, and deploying AI agents. While optimized for Gemini and the Google ecosystem, it is model-agnostic and compatible with other frameworks.15
Features: Provides a rich tool ecosystem (including pre-built tools, custom functions, OpenAPI integration), code-first development, support for modular multi-agent systems (with coordinator agents), and deployment options on Google Cloud Run or Vertex AI Agent Engine.15
ADK supports both deterministic workflow agents (sequential, parallel, loop) and LLM-driven dynamic routing for adaptive behavior.16
A notable feature is its support for MCP (Model Context Protocol) tools.16 Given that graphitti provides an MCP server 1, an ADK agent could potentially interface with the graphitti-managed KG via this protocol, creating a streamlined integration path.
While general ADK documentation exists, specific examples for RAG with multiple heterogeneous data source tools and complex retriever choice logic were not readily found in the initial research.16



LlamaIndex Agents:

LlamaIndex itself offers abstractions for building agents, often leveraging its existing data connectors, index structures, and query engines as tools.7
Agents in LlamaIndex can use QueryEngineTool to interact with different data sources (e.g., one for Neo4j, one for Supabase).17
LlamaIndex Workflows provide a way to implement more complex, multi-step agentic processes, including error handling and retries (e.g., for Text2Cypher).8
Since the core RAG pipeline is already being built with LlamaIndex, extending it with LlamaIndex agents might offer a more integrated development experience.


6.4. Recommendation and RationaleFor the initial version of this Graph RAG system, it is recommended to start with a non-agentic, LlamaIndex-orchestrated hybrid retrieval pipeline. This involves using LlamaIndex's standard retrievers (e.g., Neo4jGraphRetriever, VectorStoreIndexRetriever) and potentially a RouterRetriever or custom query pipeline to manage the flow between Neo4j and Supabase.Rationale:
Complexity Management: Building a robust hybrid RAG system with graphitti, Neo4j, Supabase, and Gemini is already a complex undertaking. Introducing an agentic layer from the outset significantly increases this complexity.
Baseline Performance: Establishing a strong baseline with a well-tuned non-agentic RAG system is crucial before evaluating the incremental benefits of an agentic approach.
LlamaIndex Capabilities: LlamaIndex's existing components (routers, query transformations, pipelines) can handle many aspects of hybrid retrieval and conditional logic without requiring a full agentic framework.
Agentic RAG should be considered a Phase 2 enhancement. Once the core system is stable and its limitations are understood, an agentic layer can be introduced to address specific shortcomings or enable more sophisticated query handling.If proceeding to Agentic RAG in a later phase:
LlamaIndex Agents would be a natural first choice due to the existing deep integration with LlamaIndex for retrieval and LLM interaction. This would likely offer a smoother development path.
Google ADK could be considered if the project requires more complex multi-agent orchestration, tighter integration with Vertex AI Agent Engine for deployment, or if the MCP integration with graphitti proves to be a significant advantage.
Table 2: Agentic RAG Implementation Approach Comparison (for future consideration)
Feature/AspectGoogle ADKLlamaIndex AgentsNotes/Recommendation (for Phase 2)Integration with LlamaIndex CoreIndirect; ADK tools would need to wrap LlamaIndex retrievers/query engines.Native; agents are built on LlamaIndex components.LlamaIndex offers tighter integration for a system already using LlamaIndex.Gemini Model SupportOptimized for Gemini; first-class support. 15Excellent support via llama-index-llms-vertex.Both are strong.Multi-Agent OrchestrationSupports multi-agent systems and various workflow agents (sequential, parallel, loop). 16Possible via LlamaIndex Workflows or custom agent compositions.ADK appears more explicitly designed for complex multi-agent hierarchies.Tool UsageRich tool ecosystem, including MCP tools, OpenAPI, custom functions. 15Flexible tool usage, primarily through QueryEngineTool and FunctionTool. 17ADK's MCP tool support is a potential advantage for graphitti. LlamaIndex tools are well-integrated with its data abstractions.Deployment on GCPDesigned for Cloud Run, Vertex AI Agent Engine. 15Deployable on any Python-supporting GCP service (Cloud Run, GKE, GCE).ADK has more specialized deployment paths within Vertex AI.Community/DocsGrowing; good documentation for core features. 16Large, active community; extensive documentation and examples. 7LlamaIndex currently has a larger user base and more readily available examples for diverse RAG scenarios.Learning CurveModerate; introduces new concepts and a specific framework.Lower if already familiar with LlamaIndex concepts.Leveraging existing LlamaIndex expertise would be faster.graphitti MCP Server IntegrationPotential direct integration via ADK's MCP tools. 16Would require a custom LlamaIndex tool to act as an MCP client.ADK might offer a more out-of-the-box solution here if graphitti MCP is central.
6.5. Key Code Examples (Conceptual for Phase 2)

Conceptual LlamaIndex agent using tools:
Python# Based on [17] (Memgraph example, adapted for Neo4j/Supabase)
# Ensure Settings.llm is configured (see section 5.4)
# from llama_index.core.tools import QueryEngineTool
# from llama_index.core.agent import ReActAgent # Or OpenAIAgent, etc.

# Assume query_engine_neo4j and query_engine_supabase are defined LlamaIndex query engines
# neo4j_tool = QueryEngineTool.from_defaults(
#     query_engine_neo4j,
#     name="neo4j_knowledge_graph_retriever",
#     description="Retrieves information about entities, relationships, and events from the Neo4j knowledge graph. Useful for specific factual lookups and understanding connections."
# )

# supabase_tool = QueryEngineTool.from_defaults(
#     query_engine_supabase,
#     name="supabase_document_vector_retriever",
#     description="Retrieves relevant text chunks from a large corpus of documents stored in Supabase. Useful for general information and semantic search over unstructured text."
# )

# agent = ReActAgent.from_tools(
#     [neo4j_tool, supabase_tool],
#     llm=Settings.llm,
#     verbose=True
# )

# response = agent.chat("Compare the financial performance of Company X in Q1 2023 (from graph) with general news sentiment about them around the same time (from documents).")
# print(response)



Conceptual Google ADK agent definition:
Python# Based on [15]
# from google.adk.agents import Agent
# from google.adk.tools import FunctionTool # Or custom tools wrapping LlamaIndex retrievers
# from llama_index.core import QueryBundle
# Assume LlamaIndex query engines (query_engine_neo4j, query_engine_supabase) are accessible

# def query_neo4j_via_llamaindex(query_str: str) -> str:
#     # response = query_engine_neo4j.query(QueryBundle(query_str))
#     # return str(response)
#     return "Mocked Neo4j response for: " + query_str


# def query_supabase_via_llamaindex(query_str: str) -> str:
#     # response = query_engine_supabase.query(QueryBundle(query_str))
#     # return str(response)
#     return "Mocked Supabase response for: " + query_str

# neo4j_adk_tool = FunctionTool(
#     name="query_knowledge_graph",
#     description="Queries the Neo4j knowledge graph for specific factual information and relationships.",
#     func=query_neo4j_via_llamaindex
# )

# supabase_adk_tool = FunctionTool(
#     name="query_document_store",
#     description="Queries the Supabase vector store for relevant document excerpts.",
#     func=query_supabase_via_llamaindex
# )

# rag_agent_adk = Agent(
#     name="hybrid_rag_assistant_adk",
#     model="gemini-2.5-pro", # From.env, ensure this is a valid Vertex AI model ID
#     instruction="You are an intelligent assistant. Answer user questions by retrieving and synthesizing information from a knowledge graph and a document vector store. Choose the appropriate tool based on the query.",
#     tools=[neo4j_adk_tool, supabase_adk_tool],
#     # Ensure project and location are configured for the Agent's LLM if using Vertex AI
#     # model_config={"project": GCP_PROJECT_ID_ENV, "location": GCP_LOCATION_ENV}
# )

# result = rag_agent_adk.process("What is the relationship betweenClearSpring and Zep AI according to the knowledge graph?")
# print(result.response_text if result else "No response from ADK agent.")


7. User Interface: Streamlit ApplicationA Streamlit application will serve as the primary user interface for interacting with the Graph RAG system. Streamlit is chosen for its rapid development capabilities and ease of creating interactive web applications for data science projects.197.1. Core Chat Functionality and Interaction FlowThe chat interface will be built using Streamlit's native chat elements:
st.chat_input: For users to type and submit their queries.22
st.chat_message: To display the conversation history, differentiating between user messages and assistant responses.22
st.session_state: To store and manage the chat history across user interactions, ensuring conversation context is maintained within a session.22
The interaction flow will be:
User enters a query in st.chat_input.
The query is appended to st.session_state.messages and displayed in the chat UI.
The Streamlit backend calls the LlamaIndex RAG pipeline with the user's query.
The RAG pipeline retrieves context, generates an answer using Gemini, and returns the response.
The assistant's response is appended to st.session_state.messages and displayed in the chat UI.
7.2. Displaying Retrieved Information and SourcesTo enhance transparency and allow users to verify the LLM's responses:
The LLM's final synthesized answer will be the primary output.
An optional, expandable section (e.g., using st.expander) will display the key pieces of retrieved context that informed the LLM's answer. This can include:

Snippets of text chunks retrieved from Supabase.
Relevant entity names, relationships, or brief summaries retrieved from the Neo4j knowledge graph.
Source document identifiers, if available.


7.3. Interactive Graph Visualization OptionsFor queries where graph relationships are particularly important, or for users who wish to explore the knowledge graph more deeply, providing graph visualization capabilities is valuable. Two main approaches are considered: embedding visualizations directly within Streamlit using Python libraries, and leveraging dedicated Neo4j graph visualization tools like Bloom.

Embedding with neo4j-viz:

The neo4j-viz Python package is designed for creating interactive graph visualizations from Neo4j data and can render its output (IPython.display.HTML) directly in Streamlit applications using st.components.v1.html.23 This library wraps the Neo4j Visualization JavaScript library (NVL).23
Features: Supports node/relationship sizing, colors, captions, pinning, tooltips, zooming, panning, and various layouts.23 It can import graphs from Neo4j query results, GDS projections, or Pandas DataFrames.23
Challenges:

neo4j-viz is still under development, and its API is subject to change, which introduces a risk of instability or bugs.23
Handling dynamic updates to visualizations in Streamlit can be complex. Typically, any change in user input that triggers a plot regeneration causes the entire plot to redraw, potentially resetting zoom and pan states.24 While Streamlit supports various charting libraries 25, truly interactive and dynamically updating graph visualizations might require careful state management or face limitations.


Alternatives like st.graphviz_chart or integrating with more mature JavaScript-based graph visualization libraries via Streamlit Components could be explored if neo4j-viz proves insufficient, though these may require more data transformation or custom development.



Leveraging Neo4j Bloom:

What it is: Neo4j Bloom is a dedicated, no-code/low-code graph data visualization and exploration tool provided by Neo4j.26 It allows both novices and experts to visually interact with and investigate Neo4j graph data without writing Cypher for basic exploration.27
Key Features:

Intuitive Exploration: Point-and-click interface for graph exploration, pattern searching, and expanding nodes.28
Perspectives: Users can create and save "perspectives" which define how data (categories, relationships, formatting, saved searches, scene actions) is displayed, allowing focus on specific aspects of the graph.30
Customization: Extensive styling options for nodes and relationships (colors, sizes, icons, captions).28
Advanced Search: Supports graph pattern searches (visual query building), custom Cypher search phrases, and full-text search.30
Data Editing: Allows visual editing of graph data.28
GDS Integration: Can run Graph Data Science algorithms and visualize results.30
Filtering and Animation: Features like the "Slicer" allow dynamic filtering and animation based on properties (e.g., time).30
Scenes and Sharing: Users can create and share specific views of the graph ("scenes") and use deep links to share specific Bloom states.31


Deployment: Bloom is available with Neo4j AuraDB instances (via the "Explore" tab in the Aura console) and Neo4j Desktop.29 It can also be deployed with a Bloom server plugin for on-premise Neo4j instances, enabling multi-user collaboration and persistent storage of perspectives.32
Licensing: Bloom is included with Neo4j AuraDB subscriptions and Neo4j Desktop.29 The neo4j-apps/neo4j-bloom GitHub repository is licensed under Apache 2.0 35, but for this project, Bloom is considered as the product feature provided with the Neo4j database.
Integration with Streamlit: Direct embedding of the full Bloom application within a Streamlit page is not its standard use case. However, the Streamlit application could:

Provide deep links 31 to pre-configured Bloom perspectives or scenes relevant to the RAG system's output or the user's query context. This would open Bloom in a separate browser tab or its own interface.
Be used as a complementary tool by analysts or power users who need more sophisticated, interactive graph exploration capabilities than what might be feasible to embed directly in Streamlit.


Bloom vs. neo4j-viz:

neo4j-viz is a developer library for embedding specific, potentially RAG-generated, subgraphs into a Python application like Streamlit.23 It requires coding to define the visualization.
Bloom is a full-fledged application for end-users (especially data analysts) to explore the entire graph interactively with a rich UI and no coding required for most operations.28 It offers a more comprehensive exploration environment.




Recommendation for Visualization:
For displaying specific, contextually relevant subgraphs generated by the RAG pipeline directly within the Streamlit chat interface, neo4j-viz (or a similar embeddable library) is the more appropriate choice, despite its developmental status. This allows for in-app visualization of the immediate evidence.
For users requiring deeper, more flexible, and code-free exploration of the entire knowledge graph, the Streamlit application should provide links or guidance on how to use Neo4j Bloom with the project's Neo4j AuraDB instance. This caters to different user needs without overcomplicating the primary chat interface.
7.4. Key Code Examples: Streamlit UI Setup and Backend Communication

Basic Streamlit chat structure:
Python# Based on [22]
import streamlit as st
import time

# Placeholder for the RAG pipeline call
# In a real app, this would import and call the LlamaIndex query engine
def call_rag_pipeline(user_query: str, chat_history: list):
    # Simulate RAG pipeline processing
    print(f"RAG Pipeline received query: {user_query}")
    print(f"Current chat history length: {len(chat_history)}")
    time.sleep(2) # Simulate delay
    # response_text = f"Based on the graph and documents, the answer to '{user_query}' is..."
    # sources =-> Entity B"}]
    # Dummy response for now
    response_text = f"I'm processing your query about: '{user_query}'. This is a placeholder response."
    sources =
    return {"answer": response_text, "sources": sources}

st.title("Graph RAG System with Gemini & Neo4j")

# Initialize chat history in session state
if "messages" not in st.session_state:
    st.session_state.messages =

# Display chat messages from history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander("View Sources"):
                for source in message["sources"]:
                    st.write(f"**Source Type:** {source.get('type', 'N/A')}")
                    st.caption(f"Content: {source.get('content', 'N/A')}")


# React to user input
if prompt := st.chat_input("Ask a question about your documents:"):
    # Add user message to history and display
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Call backend RAG system with the prompt and current history
    with st.spinner("Thinking..."):
        rag_response_data = call_rag_pipeline(prompt, st.session_state.messages)
        assistant_response_text = rag_response_data.get("answer", "Sorry, I couldn't find an answer.")
        retrieved_sources = rag_response_data.get("sources",)

    # Add assistant response to history and display
    assistant_message = {"role": "assistant", "content": assistant_response_text, "sources": retrieved_sources}
    st.session_state.messages.append(assistant_message)
    with st.chat_message("assistant"):
        st.markdown(assistant_response_text)
        if retrieved_sources:
            with st.expander("View Sources"):
                for source in retrieved_sources:
                    st.write(f"**Source Type:** {source.get('type', 'N/A')}")
                    st.caption(f"Content: {source.get('content', 'N/A')}")



Conceptual code for using neo4j-viz in Streamlit (as previously outlined):
Python# Based on [23]
# This would be part of the Streamlit app, likely triggered by a button or specific query response
# import streamlit as st
# from neo4j_viz import Node, Relationship, VisualizationGraph

# Placeholder: Function to fetch graph data (nodes, rels) from Neo4j based on RAG output
# def get_graph_data_for_visualization(rag_context_from_neo4j):
#     nodes_data =
#     rels_data =
#     # Logic to parse rag_context_from_neo4j (e.g., a list of paths or a subgraph)
#     # and convert it into neo4j_viz Node and Relationship objects.
#     # Example:
#     # nodes_data.append(Node(id="entity1", caption="Entity One", size=10, color="blue"))
#     # nodes_data.append(Node(id="entity2", caption="Entity Two", size=12, color="green"))
#     # rels_data.append(Relationship(source="entity1", target="entity2", caption="CONNECTED_TO"))
#     if not nodes_data: # Add sample data if no real data
#         nodes_data =
#         rels_data =
#     return nodes_data, rels_data

# In the main Streamlit app flow, after a RAG response:
# if st.button("Visualize Retrieved Graph Context (if any)"):
#     # Assume 'rag_response_data' contains some graph elements from Neo4j
#     # neo4j_context = extract_neo4j_elements_from_rag_response(rag_response_data) # Placeholder
#     neo4j_context = True # Simulate having context for now

#     if neo4j_context: # If there's graph data to show
#         # nodes_to_viz, rels_to_viz = get_graph_data_for_visualization(neo4j_context)
#         # For demonstration, using placeholder function directly:
#         nodes_to_viz, rels_to_viz = get_graph_data_for_visualization(None)


#         if nodes_to_viz:
#             try:
#                 graph_visualization = VisualizationGraph(
#                     nodes_to_viz,
#                     rels_to_viz,
#                     directed=True, # Or False, depending on graph nature
#                     # Other options: layout, node_label_property, edge_label_property etc.
#                 )
#                 # The render() method returns an IPython.display.HTML object.
#                 # Streamlit's st.components.v1.html can render this.
#                 html_output = graph_visualization.render().data
#                 st.components.v1.html(html_output, height=600, scrolling=True)
#             except Exception as e:
#                 st.error(f"Failed to render graph visualization: {e}")
#         else:
#             st.info("No specific graph elements were retrieved for visualization in this response.")
#     else:
#         st.info("No graph context available to visualize for the last query.")


Streamlit's rapid development cycle makes it suitable for quickly building a functional UI. However, for advanced interactive visualizations like dynamic graph updates, careful consideration of its limitations and the maturity of chosen libraries like neo4j-viz is necessary. Complementing this with access to a powerful tool like Neo4j Bloom for deeper analysis provides a balanced approach.8. Google Cloud Platform (GCP) Integration and Deployment StrategyThe system leverages several Google Cloud Platform services, primarily for hosting the Neo4j AuraDB (which can run on GCP infrastructure), running the LLM and embedding models via Vertex AI, and potentially deploying the application components.8.1. Neo4j AuraDB on GCPThe project utilizes an existing Neo4j AuraDB instance: neo4j+s://037bf0a4.databases.neo4j.io (Image 1). Neo4j AuraDB is a fully managed cloud graph database service that can be deployed on GCP. Integration primarily involves ensuring network connectivity from application components (e.g., graphitti server, LlamaIndex backend) running on GCP to the AuraDB endpoint. Standard security practices, such as IP allowlisting and secure credential management, should be followed.8.2. graphitti MCP Server Deployment on GCPgraphitti includes a Model Context Protocol (MCP) server and a REST API server built with FastAPI, which can be used for interacting with the knowledge graph.1 The mcp_server can be deployed using Docker 6, making it suitable for various GCP container services:
Google Cloud Run: Ideal for stateless, containerized applications. If the graphitti MCP server or REST API can be run as stateless instances (with Neo4j AuraDB as the external stateful backend), Cloud Run offers serverless deployment, auto-scaling, and pay-per-use pricing.
Google Kubernetes Engine (GKE): For more complex deployments or if managing multiple graphitti instances (e.g., using the mcp-graphiti multi-server pattern 20), GKE provides a robust orchestration platform. Docker Compose configurations from mcp-graphiti can be adapted into Kubernetes manifests.
Google Compute Engine (GCE): Virtual machines on GCE offer maximum control but require more manual setup and management. This approach might involve configuring VPCs, DNS, Load Balancers, and Instance Groups, similar to deploying other database or backend services on VMs.36
The mcp-graphiti repository 20 proposes a Docker Compose-based deployment for managing multiple graphitti MCP server instances with a shared Neo4j database. This pattern, promoting project isolation and easier management, is well-suited for translation to GKE deployments or multiple Cloud Run services.8.3. Utilizing Other GCP Services
Vertex AI: This is a core component, used for accessing Google Gemini models for generation (gemini-2.5-pro) and embedding (text-embedding-004 or the specified gemini-2.5-flash if compatible) as defined in the .env and system design (User Query, Image 1).
Cloud Storage (GCS): While the .env files (Image 1, Image 2) mention a DRIVE_FOLDER_ID (implying Google Drive), for production-grade document storage, GCS is recommended. GCS buckets can store raw documents before ingestion by graphitti, serve as a backup location for the Neo4j graph data (analogous to GraphDB backups to Cloud Storage 36), or store other large artifacts.
Identity and Access Management (IAM): Essential for managing permissions for all GCP resources. Service accounts with least-privilege access should be configured for the graphitti server, the LlamaIndex backend application, and any components interacting with Vertex AI or other GCP services. The IMPERSONATED_USER_EMAIL (kevin@clearspringcg.com from Image 1, Image 2) indicates the use of service account impersonation, a good practice for fine-grained access control.
Cloud DNS and Load Balancing: If deploying graphitti services (MCP, REST) or the Streamlit application backend on GCE/GKE for high availability, custom domain mapping, or SSL termination, Cloud DNS and Cloud Load Balancing would be necessary.36
Secret Manager: For securely storing sensitive credentials like API keys and database passwords, rather than relying solely on .env files in production environments.
While graphitti itself primarily highlights OpenAI integration in its main documentation 1, the graphiti-core[google-genai] package specifically enables Gemini support.1 Thus, graphitti's primary GCP integration point is its use of Vertex AI for LLM and embedding tasks. Further integrations, such as using Pub/Sub for event-driven ingestion into graphitti or Dataflow for large-scale preprocessing, would be custom application-level developments around graphitti rather than built-in features.A security consideration from the .env (Image 1) is the Neo4j AuraDB username wayne.painters@outlook.com. For production systems, it is strongly recommended to use dedicated, non-personal service accounts or roles for database access to improve security and manageability.9. Consolidated Technical SpecificationsThis section consolidates key technical details, including environment configurations and data flow diagrams, to provide a unified reference.9.1. Comprehensive Environment Variables and ConfigurationThe system relies on numerous environment variables for configuring access to various services. These are primarily sourced from the .env files provided (Image 1, Image 2).Table 3: Consolidated Environment VariablesVariable NameDescriptionSource (.env Image)Example Value (Masked/Generic)SUPABASE_URLURL of the Supabase project.Image 1https://odpykcmtcwmyfolsrgcu.supabase.coSUPABASE_KEYService role key for Supabase API access.Image 1eyJh...XVCJ9.eyJp...ImNnIl0.ey...Y3MSUPABASE_PROJECT_IDUnique identifier for the Supabase project.Image 1odpykcmtcwmyfolsrgcuSUPABASE_DATABASE_PASSWORDPassword for the Supabase PostgreSQL database.Image 1HlabaysirdtgycuSUPABASE_DATABASE_NAMEName of the Supabase PostgreSQL database.Image 1postgresNEO4J_URIConnection URI for Neo4j AuraDB.Image 1neo4j+s://037bf0a4.databases.neo4j.ioNEO4J_USERUsername for Neo4j AuraDB.Image 1wayne.painters@outlook.comNEO4J_PASSWORDPassword for Neo4j AuraDB.Image 1o1ocsUT...wZgkNEO4J_DATABASEName of the Neo4j database.Image 1neo4jAURA_INSTANCEIDInstance ID for Neo4j AuraDB.Image 1037bf0a4AURA_INSTANCENAMEInstance Name for Neo4j AuraDB.Image 1Instance01GCP_PROJECT_IDGoogle Cloud Project ID for Vertex AI and other GCP services.Image 1neo4j-deployment-new1GCP_LOCATIONGoogle Cloud region/location for Vertex AI services.Image 1us-central1EMBED_MODEL_LLMIdentifier for the embedding model (Gemini on Vertex AI).Image 1models/gemini-2.5-flashFALLBACK_LLM_MODELIdentifier for the fallback LLM (Gemini on Vertex AI).Image 1gemini-2.5-flashLLM_MODELIdentifier for the primary LLM (Gemini on Vertex AI).Image 1gemini-2.5-proANTHROPIC_API_KEYAPI Key for Anthropic models (Optional, if used).Image 1, Image 2sk-ant-api03-...YiwAAPERPLEXITY_API_KEYAPI Key for Perplexity models (Optional, if used).Image 1, Image 2pplx-...N3H4OPENAI_API_KEYAPI Key for OpenAI models (Optional, if used as alternative/fallback).Image 1, Image 2sk-proj-...GR2LKAGOOGLE_API_KEYGeneral Google API Key (may be used by clients if ADC not primary).Image 1, Image 2AIzaSyC...gNUKGOOGLE_CLOUD_PROJECTGoogle Cloud Project ID (alternative name for GCP_PROJECT_ID).Image 2neo4j-genai-test-projectGOOGLE_CLOUD_LOCATIONGoogle Cloud region/location (alternative name for GCP_LOCATION).Image 2us-central1DRIVE_FOLDER_IDGoogle Drive Folder ID for input documents.Image 1, Image 21vbJ-5VnV_gTlegoW0bO0A26gR7rwJDajmIMPERSONATED_USER_EMAILEmail for service account impersonation on GCP.Image 1, Image 2kevin@clearspringcg.comLLAMA_CLOUD_API_KEYAPI Key for LlamaCloud (Optional, if LlamaParse or other services used).Image 2llx-YenT...BcDyLA consolidated and centrally managed approach to these configurations, potentially using GCP Secret Manager for production secrets, is crucial for system stability and security. Discrepancies, such as the two different GCP project IDs (neo4j-deployment-new1 vs. neo4j-genai-test-project), must be resolved.9.2. Data Flow DiagramsDiagram 2: Data Ingestion Flow (Source Documents -> graphitti -> Neo4j AuraDB)Code snippetgraph LR
    A --> B{Document Loader/Parser};
    B --> C{graphitti Core Engine};
    C -- Uses --> D[Gemini LLM via Vertex AI for Extraction];
    C -- Uses --> E[Gemini Embedding Model via Vertex AI];
    D -- Extracted Entities/Relationships --> C;
    E -- Embeddings --> C;
    C -- Formatted Graph Data (Nodes, Edges with Temporal Info & Embeddings) --> F;
    F -- Stores --> G[Knowledge Graph];
This diagram illustrates how documents are processed by graphitti, leveraging Gemini models for entity/relationship extraction and embedding, and finally populating the Neo4j AuraDB knowledge graph with temporally-aware data.Diagram 3: Query Flow (User via Streamlit -> LlamaIndex Orchestrator -> Hybrid Retrievers -> LLM -> Streamlit)Code snippetgraph TD
    U[User] -- Query --> SO{Streamlit Backend};
    SO -- Processed Query --> LO{LlamaIndex Orchestrator};
    LO -- Graph Retrieval Request --> NR;
    NR -- Cypher/Keyword/Vector Query --> NDB;
    NDB -- Graph Context (Entities, Relations, Text) --> NR;
    NR -- Retrieved Graph Context --> LO;
    LO -- Vector Retrieval Request --> SR;
    SR -- Vector Query --> SDB;
    SDB -- Document Chunks --> SR;
    SR -- Retrieved Vector Context --> LO;
    LO -- Combined Context + Prompt --> LLM[Gemini LLM via Vertex AI];
    LLM -- Generated Answer --> LO;
    LO -- Final Response + Sources --> SO;
    SO -- Display Answer & Sources --> U;
This diagram shows the path of a user query from the Streamlit interface, through LlamaIndex which orchestrates hybrid retrieval from Neo4j and Supabase, to the Gemini LLM for answer synthesis, and back to the user.9.3. API Endpoint DefinitionsInternal APIs will exist between components:
Streamlit Backend to LlamaIndex Orchestrator: A Python API call to trigger the RAG pipeline.
LlamaIndex to Neo4j/Supabase: Handled by LlamaIndex data connectors using database drivers/APIs.
LlamaIndex to Vertex AI: HTTPS API calls to Vertex AI endpoints for Gemini models.
graphitti to Neo4j: graphitti uses the Neo4j Python driver.
graphitti to Vertex AI: HTTPS API calls for its internal LLM/embedding needs.
If the graphitti REST service or MCP server is deployed and exposed, its API endpoints (e.g., for adding episodes, querying graph elements) would be defined by graphitti's FastAPI application or MCP specification.1 For this project, direct interaction with graphitti's servers by the user-facing application is less likely; LlamaIndex will be the primary interface to the data stores populated by graphitti.10. Future Considerations and EnhancementsWhile the current PRD outlines a comprehensive Graph RAG system, several areas offer potential for future enhancements and require ongoing attention.10.1. Scalability, Performance Optimization, and Cost Management
Scalability:

Neo4j AuraDB: Monitor performance and scale the AuraDB instance tier as data volume and query complexity grow. AuraDB offers different tiers to accommodate varying workloads.34
Supabase: Monitor PostgreSQL performance and consider scaling options if the vector store becomes a bottleneck.
GCP Services: Cloud Run and GKE offer auto-scaling for stateless application components. Vertex AI endpoints for Gemini models are managed services designed for scalability.


Performance Optimization:

graphitti Ingestion: Optimize batch sizes and LLM calls during graphitti's data ingestion process. graphitti is designed for scalability with large datasets and parallelizing LLM calls.1
LlamaIndex Pipelines: Profile and optimize LlamaIndex retrieval and synthesis pipelines. This includes tuning retriever parameters (e.g., similarity_top_k), prompt engineering, and response synthesizer choices.
Query Optimization: For Neo4j, ensure appropriate indexes are in place (beyond what graphitti creates, if needed) and optimize Cypher queries generated by TextToCypher or custom logic.
Caching: Implement caching at various levels (e.g., retrieved context, LLM responses for common queries, embedding results) to reduce latency and cost.


Cost Management:

LLM Calls: Monitor token usage for Gemini models (extraction, embedding, synthesis, agentic reasoning if implemented). Employ strategies like using smaller/faster models (e.g., Flash versions) for less critical tasks, prompt optimization to reduce token count, and response length limits.
GCP Resource Utilization: Regularly review GCP costs for AuraDB, Supabase (if hosted on GCP), Vertex AI, GCS, and compute services. Utilize GCP's cost management tools.


The performance of graphitti's ingestion and LlamaIndex's retrieval from Neo4j will be critical as the knowledge graph expands. While graphitti's hybrid indexing aims for efficient access 1, large and complex graphs can still present challenges for specific query patterns. Continuous performance monitoring and optimization will be essential.10.2. System Evaluation and Monitoring Frameworks
RAG Quality Evaluation:

Implement metrics to assess the quality of the RAG system, such as:

Answer Relevance: How well the generated answer addresses the user's query.
Faithfulness/Attribution: Whether the answer is grounded in the retrieved context and avoids hallucination.
Context Recall/Precision: How effectively the retrieval system fetches relevant context and avoids irrelevant information.


LlamaIndex provides evaluation modules that can be adapted for these purposes.9 Frameworks like RAGAs or custom evaluation suites can also be used.


System Monitoring:

Utilize GCP's Cloud Monitoring for system health, API latencies, error rates, and resource utilization of components deployed on GCP.
Monitor Neo4j AuraDB and Supabase performance through their respective dashboards and logging.
Log key interactions and decisions within the LlamaIndex pipeline for debugging and analysis.


10.3. Advanced graphitti Feature Explorationgraphitti offers several advanced capabilities that could be explored in future iterations:
Deeper Temporal Queries: Leverage graphitti's bi-temporal model to answer historical questions, analyze trends over time, or reconstruct the state of knowledge at specific points in time.1 This is a core differentiator and could unlock significant value beyond basic RAG.
Community Detection: graphitti has capabilities related to community detection within the graph. Exploring how these communities could inform retrieval or provide higher-level insights might be beneficial.
Custom Graph Schemas: While graphitti can work with Pydantic models for custom entities, further exploration of its support for more complex, developer-defined node and edge classes could allow for even more tailored knowledge representation.1
Conflict Resolution: Investigate graphitti's mechanisms for handling conflicting information when ingesting new data, particularly how it uses temporal metadata to update or invalidate outdated facts.3
10.4. Enhanced Agentic CapabilitiesIf the initial RAG system proves successful, revisiting Agentic RAG (as discussed in Section 6) with a focus on specific improvements:
Dynamic Tool Selection: Agents that can choose between graph queries, vector searches, or even external APIs based on query analysis.
Multi-Hop Reasoning: Agents capable of performing sequences of queries (e.g., traversing multiple relationships in the graph) to answer complex questions.
Self-Correction/Refinement: Agents that can evaluate the results of a retrieval step and decide to retry with a modified query or strategy if the initial context is insufficient.
11. ConclusionThis Product Requirements Document details the technical design for a sophisticated Graph RAG system. By integrating graphitti-core for dynamic knowledge graph creation in Neo4j AuraDB, leveraging LlamaIndex for hybrid retrieval from the graph and a Supabase vector store, and utilizing Google's Gemini models via Vertex AI, the system aims to provide accurate, context-aware answers through a Streamlit-based chat interface.The emphasis on graphitti's temporal capabilities and custom entity definitions provides a strong foundation for a KG that evolves with new information. The hybrid retrieval strategy ensures that both structured, interconnected knowledge and broad semantic context can be utilized. The Streamlit UI will provide core chat functionality, with options for embedded graph visualizations using libraries like neo4j-viz for immediate context, and guidance for using powerful external tools like Neo4j Bloom for deeper, code-free graph exploration by analysts. While Agentic RAG presents an avenue for future enhancement, the initial focus will be on establishing a robust and performant core RAG pipeline.Successful implementation will depend on careful attention to model configuration (particularly embedding model alignment), efficient data ingestion and retrieval strategies, and robust GCP deployment. The provided technical specifications and code examples offer a starting point for development, with ongoing evaluation and iteration being key to achieving the project's goals.



######



Updated Graph RAG Project Build Plan: Google Drive Data Ingestion Pipeline1. IntroductionThis document outlines an update to the Graph RAG project build plan, incorporating a new, on-demand data ingestion pipeline. The primary objective of this enhancement is to enable the system to fetch documents, including PDFs and various other file types, from a specified Google Drive folder upon user request. These documents will be processed by LlamaParse via the Llama Cloud API, utilizing the "balanced" parsing strategy to generate structured JSON output. This JSON data will subsequently be ingested by graphitti to populate and enrich the Neo4j knowledge graph.All other foundational components of the existing Graph RAG system—including the Neo4j AuraDB for graph storage, Supabase for supplementary vector embeddings, Google Gemini models accessed via Vertex AI for generative tasks, and the Streamlit user interface—will be maintained and integrated with this new parsing and ingestion workflow. This update aims to significantly enhance the dynamism and scope of the knowledge available to the RAG system by incorporating user-specified documents directly into the knowledge graph.12. Current System Architecture OverviewThe existing Graph RAG system architecture integrates several key technologies to provide a hybrid retrieval and generation capability:
Neo4j AuraDB: Serves as the primary persistent store for the knowledge graph. Configuration parameters such as NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, and NEO4J_DATABASE are managed through the .env file.
Supabase: Utilized for storing vector embeddings, enabling a hybrid RAG approach that combines graph-based retrieval with semantic vector search. Connection details like SUPABASE_URL and SUPABASE_KEY are also defined in the .env file.3
graphitti-core: The core engine responsible for constructing, managing, and querying the temporal knowledge graph within Neo4j. It plays a crucial role in how unstructured and structured data is transformed into graph representations.5
LlamaIndex: Acts as an orchestration framework, facilitating the interaction between various components, including data loaders, indexers, retrievers, and LLMs.7
Google Gemini Models via Vertex AI: The primary Large Language Models (LLMs) used for understanding queries, generating responses, and other AI-driven tasks. Specific models like gemini-2.5-pro-preview and gemini-2.5-flash-preview (based on .env variables like LLM_MODEL and FALLBACK_LLM_MODEL) are accessed through Google Cloud's Vertex AI platform. Configuration relies on GCP_PROJECT_ID and GCP_LOCATION from the .env file.9
Streamlit: Provides the interactive user interface, enabling users to submit queries, view results, and, with this update, trigger data ingestion processes.11
Hybrid RAG Approach: The system combines the strengths of knowledge graph traversal (via Neo4j and graphitti) and vector similarity search (via Supabase) to retrieve relevant context for the LLMs, aiming for more accurate and comprehensive answers.14
3. Analysis of .env Configuration and DiscrepanciesThe .env file contains critical configuration variables for the project. An analysis of the provided images (Image 1 and Image 2) reveals the following key variables and one notable discrepancy:VariablePurposeValue (from Image 1 / Image 2)Source Image(s)SUPABASE_URLSupabase project API URLhttps://odpykcdmtcwmyfolrsgcu.supabase.coImage 1SUPABASE_KEYSupabase project public API key (anon key)eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc...Image 1NEO4J_URINeo4j AuraDB connection URIneo4j+s://037bf8a4.databases.neo4j.ioImage 1NEO4J_USERNeo4j database usernameneo4jImage 1NEO4J_PASSWORDNeo4j database passwordo1ocsUT2c2yE-382nkMHwppYeK6z3YBIxVm2Lw2ZgkImage 1NEO4J_DATABASENeo4j database nameneo4jImage 1AURA_INSTANCEIDNeo4j AuraDB instance ID037bf8a4Image 1AURA_INSTANCENAMENeo4j AuraDB instance nameInstance01Image 1GCP_PROJECT_IDGoogle Cloud Project IDImage 1: neo4j-deployment-new1 <br> Image 2: neo4j-genai-test-projectImage 1, Image 2GCP_LOCATIONGoogle Cloud Project Locationus-central1Image 1, Image 2EMBED_MODEL_LLMPrimary embedding modelgemini-2.5-flashImage 1FALLBACK_LLM_MODELFallback LLM for generationgemini-2.5-proImage 1LLM_MODELPrimary LLM for generationgemini-2.5-proImage 1ANTHROPIC_API_KEYAPI key for Anthropic modelssk-ant-api03-...Image 1, Image 2PERPLEXITY_API_KEYAPI key for Perplexity modelspplx-...Image 1, Image 2OPENAI_API_KEYAPI key for OpenAI modelssk-proj-...Image 1, Image 2GOOGLE_API_KEYAPI key for Google AI servicesAIzaSyChzJNQ...Image 1, Image 2GOOGLE_CLOUD_PROJECTGoogle Cloud Project ID (duplicate of GCP_PROJECT_ID)neo4j-genai-test-projectImage 2GOOGLE_CLOUD_LOCATIONGoogle Cloud Project Location (duplicate of GCP_LOCATION)us-central1Image 2DRIVE_FOLDER_IDGoogle Drive Folder ID for document fetching1vbJ-5VnV_gtIegoW0b0A26gR7rwJDajmImage 1, Image 2IMPERSONATED_USER_EMAILEmail for Google Drive impersonation (if used)kevin@clearspringcg.comImage 1, Image 2LLAMA_CLOUD_API_KEYAPI key for Llama Cloud services (LlamaParse)llx-YenT5kJS...Image 2A critical discrepancy exists for the GCP_PROJECT_ID. Image 1 lists it as neo4j-deployment-new1, while Image 2 (which appears to be a more recent or focused view of API keys) lists it as neo4j-genai-test-project (also duplicated as GOOGLE_CLOUD_PROJECT). This inconsistency must be resolved to ensure reliable access to Google Cloud services, particularly Vertex AI for Gemini models and the Google Drive API for the new ingestion pipeline. The value from Image 2 (neo4j-genai-test-project) should be confirmed as the correct one for all GCP-related services.The LLAMA_CLOUD_API_KEY is essential for the new pipeline, as it authenticates requests to the LlamaParse service.15 Similarly, DRIVE_FOLDER_ID specifies the target folder in Google Drive, and IMPERSONATED_USER_EMAIL indicates a potential configuration for service account impersonation or a specific user context for accessing Drive files, which will require careful setup of OAuth 2.0 credentials and permissions.174. Detailed Design of the New Data Ingestion Pipeline4.1. Overview and ObjectivesThe new data ingestion pipeline is designed to operate on user demand. Its core objectives are:
Fetch a diverse range of documents (including PDFs, DOCX, PPTX, etc.) from a user-specified or pre-configured Google Drive folder.
Process these documents using the LlamaParse service, invoked via the Llama Cloud API. The "balanced" parsing strategy will be employed, and the desired output format is structured JSON.
Ingest the structured JSON data into the Neo4j AuraDB instance using the graphitti-core library, thereby enriching the existing knowledge graph.
4.2. Component BreakdownThe pipeline consists of three primary stages:4.2.1. Google Drive Document FetchingThis stage is responsible for accessing and retrieving files from the designated Google Drive folder.
Technology: The llama-index-readers-google package, specifically its GoogleDriveReader class, is recommended for this purpose.17
Authentication: Access to Google Drive will be managed through OAuth 2.0. Client credentials (typically a credentials.json file obtained from the Google Cloud Console) must be configured. The presence of IMPERSONATED_USER_EMAIL in the .env file suggests that the OAuth consent screen or service account setup might need to allow for impersonation or operate under this specific user's context to access the files. This requires careful configuration to ensure appropriate permissions and security.17
Input: The DRIVE_FOLDER_ID environment variable will specify the target folder.
Output: The content of the fetched documents (e.g., binary data for PDFs, text for other formats) will be passed to the next stage. GoogleDriveReader can load various common file types.
4.2.2. LlamaParse Processing (Llama Cloud API)Once documents are fetched, LlamaParse will process them to extract structured information.
API Key: The LLAMA_CLOUD_API_KEY from the .env file will authenticate requests to the Llama Cloud API.15
Library: The llama-parse Python client library will be used to interact with the API.15
Parsing Strategy: The "balanced" strategy is specified. LlamaParse offers several presets, where "balanced" is the default, aiming for an optimal trade-off between parsing accuracy, layout preservation, and processing performance. This strategy is suitable for a mix of document types.19 Should this strategy prove insufficient for highly complex or visually rich documents, alternatives like the "premium" preset could be considered, though this may have cost and latency implications.
Output Format: The pipeline requires structured JSON output. This can be achieved by setting result_type="json" in the LlamaParse client or by using the structured_output=True parameter along with a schema definition.16 LlamaParse supports predefined schemas like "imFeelingLucky" (which infers output format), "invoice," and "resume," or allows for a custom JSON schema to be provided.20 For general document types from Google Drive, initiating with "imFeelingLucky" or a broadly defined custom schema is a practical starting point. The quality and consistency of this JSON output are paramount for successful graph ingestion.
Input: Document content (file paths or binary data) obtained from the Google Drive fetching stage. LlamaParse supports various input methods, including direct file uploads and URLs.21
4.2.3. graphitti Ingestion into Neo4jThe structured JSON from LlamaParse will be used to populate the Neo4j knowledge graph via graphitti.
Library: The graphitti-core Python library will be used.5
Input: A list of structured JSON objects, where each object represents a parsed document or sections thereof.
Mechanism: graphitti is designed to ingest structured data, including JSON, and transform it into graph structures (nodes and relationships) within Neo4j. A key feature of graphitti is its use of Pydantic models to define the expected schema of the input JSON and to guide the mapping process to graph entities and relationships.5 The graphitti quickstart documentation explicitly mentions its capability to add episodes from structured JSON.5 The MCP server built on graphitti-core uses an add_episode function with source="json" 23, indicating a similar capability in the core library.
Temporal Awareness: A significant strength of graphitti is its bi-temporal data model, which tracks both the event occurrence time and ingestion time. This allows for the creation of a dynamic knowledge graph that can represent changes and maintain historical context without requiring full recomputation for updates.14
Output: The Neo4j knowledge graph will be populated with new nodes and relationships derived from the ingested documents.
The definition of appropriate Pydantic models within the graphitti ingestion script is a critical development task. These models must accurately reflect the structure of the JSON produced by LlamaParse to ensure meaningful graph construction. For nested JSON structures, these Pydantic models would themselves be nested or reference other models.25
4.3. Data Flow DiagramThe data flow for the new ingestion pipeline can be summarized as follows:
User Demand: Initiated via the Streamlit interface or a CLI command.
Google Drive Fetcher: Uses DRIVE_FOLDER_ID and OAuth credentials (potentially with IMPERSONATED_USER_EMAIL) to access and download documents.
Document Content: Raw document data is passed to LlamaParse.
LlamaParse Service: Processes documents using LLAMA_CLOUD_API_KEY, the "balanced" strategy, and configured for structured JSON output.
Structured JSON: The parsed, structured data is returned.
graphitti Ingester: Uses Pydantic models to interpret the JSON and Neo4j credentials from .env to write nodes and relationships to the Neo4j knowledge graph.
Neo4j Knowledge Graph: The graph is updated with new information.
4.4. On-Demand Trigger MechanismThe pipeline must be triggerable on user demand.
Initial Implementation: A command-line interface (CLI) script will be developed to trigger the entire pipeline. This allows for straightforward testing and execution during development.
Future Enhancement: An API endpoint (e.g., using FastAPI) will be created. This endpoint can then be called from the Streamlit user interface, allowing users to initiate the ingestion process directly from the application (e.g., by specifying a Google Drive folder or confirming a pre-set one).
Consideration must be given to handling potentially concurrent requests if multiple users can trigger the ingestion process simultaneously, possibly involving a queuing mechanism or ensuring the pipeline components are thread-safe or can be instantiated per request.
5. Integration with Existing System ComponentsThe new data ingestion pipeline will integrate with and augment the capabilities of the existing system components:
5.1. Neo4j AuraDB: The primary beneficiary of the new pipeline. New nodes, relationships, and properties extracted from the Google Drive documents will be added to the Neo4j knowledge graph by graphitti. Careful consideration must be given to the graph schema. graphitti can automatically infer an ontology or utilize custom-defined entity types based on Pydantic models.22 The new data should seamlessly extend the existing schema or introduce new, well-defined structures.
5.2. Supabase Vector Store: The current request focuses on populating the knowledge graph. The existing RAG system's reliance on Supabase for vector embeddings of other data sources will remain unchanged. A potential future enhancement could involve taking the textual content from the LlamaParse JSON output, embedding it, and storing these embeddings in Supabase. This would allow the newly ingested documents to be immediately available for hybrid (graph + vector) search strategies. However, this is outside the scope of the immediate build plan update.
5.3. LlamaIndex Orchestration: LlamaIndex will continue to orchestrate the RAG query process. The enriched knowledge graph in Neo4j will provide a more comprehensive source of context for LlamaIndex's retrievers. The new ingestion pipeline itself may operate as a separate process, triggered on demand, rather than being part of LlamaIndex's direct query-time data loading, although components like GoogleDriveReader are LlamaIndex compatible.18
5.4. Google Gemini Models (Vertex AI): Gemini models will continue to be the core intelligence for generating responses in the RAG system. The enhanced context retrieved from the more comprehensive Neo4j graph (populated by the new pipeline) is expected to improve the accuracy, relevance, and depth of the responses generated by Gemini.
5.5. Streamlit Interface: The Streamlit application will remain the primary user interaction point. A new UI element (e.g., a button or form) will be required to allow users to trigger the on-demand data ingestion from Google Drive. Users should observe improved RAG performance and the ability to query information from newly ingested documents through the existing chat interface.11 Feedback mechanisms within Streamlit regarding the status of the ingestion process (e.g., "Processing started," "X documents ingested successfully," "Error during parsing") would enhance user experience.
6. Implementation Plan & Key Script Components (Conceptual)The implementation will involve creating several Python scripts and ensuring their orchestration.

6.1. Google Drive Document Retrieval Script:

This script will utilize the llama-index-readers-google library, specifically the GoogleDriveReader class.18
It will handle OAuth 2.0 authentication using credentials.json and consider the IMPERSONATED_USER_EMAIL for appropriate access scopes.
The script will accept a folder_id (from DRIVE_FOLDER_ID in .env) and iterate through the files, loading their content. It should be capable of handling various file types supported by GoogleDriveReader and LlamaParse.
A conceptual snippet:
Python# drive_retriever.py
import os
from llama_index.readers.google import GoogleDriveReader

def fetch_documents_from_drive(folder_id: str) -> list:
    # Ensure 'credentials.json' is correctly placed and configured as per LlamaIndex/Google API docs.
    # The GoogleDriveReader handles OAuth flow if credentials.json is present and valid.
    # It might require user interaction for the first-time OAuth approval if not using a service account.
    loader = GoogleDriveReader()
    # load_data can take a folder_id or a list of folder_ids
    documents = loader.load_data(folder_id=folder_id)
    # 'documents' will be a list of LlamaIndex Document objects.
    # Content might need to be extracted or passed appropriately to LlamaParse.
    # For LlamaParse, we might need file paths or binary content.
    # This step may involve saving files temporarily if LlamaParse client requires file paths.
    # Or, if GoogleDriveReader provides in-memory content, that can be used.
    # For simplicity, assuming LlamaParse can take file paths downloaded by GoogleDriveReader or their content.
    print(f"Fetched {len(documents)} documents from Google Drive folder: {folder_id}")
    return documents # Or list of file paths / content

.18load_data(folder_id=folder_id)`). This script needs to ensure it provides document data in a format LlamaParse can consume (e.g., file paths to temporarily saved files or binary data).



6.2. LlamaParse API Interaction Script:

This script will use the llama-parse Python client library to send document content to the Llama Cloud API.
It will be configured with the LLAMA_CLOUD_API_KEY.
The "balanced" parsing strategy will be specified, and the output requested as "structured JSON".
A conceptual snippet:
Python# llama_parser_client.py
import os
from llama_parse import LlamaParse

def parse_documents_with_llamaparse(file_paths_or_binary_data: list) -> list:
    llama_cloud_api_key = os.getenv("LLAMA_CLOUD_API_KEY")
    if not llama_cloud_api_key:
        raise ValueError("LLAMA_CLOUD_API_KEY not found in environment variables.")

    parser = LlamaParse[20]

    # LlamaParse's load_data can accept a list of file paths or potentially binary data.
    # This example assumes file_paths.
    parsed_json_outputs =
    # Depending on LlamaParse library, it might support batch or require iteration
    for file_path in file_paths_or_binary_data: # Assuming file_paths for this example
         # This is a simplified call; actual usage might involve async or batch methods
        try:
            # load_data returns a list of LlamaIndex Document objects if result_type is markdown/text
            # For result_type="json", the structure of 'documents' needs to be confirmed from LlamaParse docs.
            # It might be a list of dicts, or LlamaIndex Document objects with JSON in metadata.
            # Assuming it returns a list of parsed results, where each result corresponds to a file.
            # The LlamaParse library might return a list of JobResult objects, each containing pages with structuredData.
            job_results = parser.load_data([file_path]) # parser.load_data expects a list of paths
            for job_result in job_results: # Assuming job_results is a list of results per file
                # Accessing structured JSON, actual structure depends on LlamaParse version and result_type="json"
                # [20] (structured_output) implies the JSON is directly available.
                # For example, if job_result is a dict:
                if isinstance(job_result, dict) and "structured_content" in job_result: # Hypothetical key
                     parsed_json_outputs.append(job_result["structured_content"])
                # Or if it's an object with attributes:
                # elif hasattr(job_result, 'json_output'):
                #    parsed_json_outputs.append(job_result.json_output)
                # The exact way to get the JSON needs to be verified with LlamaParse documentation for result_type="json"
                # and structured_output=True.
                # For now, let's assume `parser.load_data` with `result_type="json"` returns list of JSON serializable dicts.
                # Or, if it returns LlamaIndex Documents, the JSON might be in `doc.metadata['structured_json']` or similar.
                # The example from [15] suggests it returns LlamaIndex Documents.
                # If so, the JSON might be embedded within these Document objects.
                # A more robust approach would be to inspect the objects returned by parser.load_data
                # when result_type="json" or structured_output=True is used.
                # For now, assuming it's a list of JSON strings or dicts.
                # Let's assume job_results are directly the JSON outputs for simplicity here.
                parsed_json_outputs.extend(job_results) # Placeholder for actual extraction logic
        except Exception as e:
            print(f"Error parsing document {file_path} with LlamaParse: {e}")

    print(f"Successfully parsed {len(parsed_json_outputs)} documents using LlamaParse.")
    return parsed_json_outputs

15. The exact method to specify the "balanced" strategy needs to be confirmed from LlamaParse documentation if it's not the implicit default when no other strategy is chosen. The structure of the returned JSON when result_type="json" is used also needs to be carefully examined to ensure it's directly usable by graphitti.



6.3. graphitti Ingestion Script:

This script will use graphitti-core to ingest the structured JSON from LlamaParse into Neo4j.
It will require defining Pydantic models that mirror the expected structure of the LlamaParse JSON output. These models guide graphitti in creating nodes and relationships.
The script will initialize a Graphiti client with Neo4j connection details from the .env file.
A conceptual snippet:
Python# graphitti_ingester.py
import os
import json # For handling JSON strings if LlamaParse returns them as strings
from graphiti_core import Graphiti # Assuming this is the correct import path
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any

# Define Pydantic models based on the expected LlamaParse structured JSON output.
# This is a generic example; actual models will depend on LlamaParse's output for "imFeelingLucky"
# or a more specific schema if one is used.
class ExtractedEntity(BaseModel):
    label: str
    name: str
    properties: Optional] = None

class ExtractedRelationship(BaseModel):
    source_entity_label: str
    source_entity_name: str
    target_entity_label: str
    target_entity_name: str
    type: str
    properties: Optional] = None

class LlamaParseOutputModel(BaseModel):
    document_id: str
    title: Optional[str] = None
    summary: Optional[str] = None
    entities: List[ExtractedEntity] =
    relationships: List =
    raw_text_chunks: Optional[List[str]] = None # Example field
    # Add other fields as expected from LlamaParse's "imFeelingLucky" JSON output

def ingest_data_with_graphitti(json_outputs: list, group_id: str = "default_drive_ingest"):
    graphiti_instance = Graphiti(
        url=os.getenv("NEO4J_URI"),
        user=os.getenv("NEO4J_USER"),
        password=os.getenv("NEO4J_PASSWORD"),
        # group_id might be set here or per operation
    )
    # Ensure indices and constraints are set up (Graphiti might handle this on init or require explicit call)
    # graphiti_instance.setup_indices_constraints() # Example, check Graphiti docs

    for i, json_data_item in enumerate(json_outputs):
        try:
            # Assuming json_data_item is a Python dict or can be loaded into one.
            # If it's a string, parse it:
            if isinstance(json_data_item, str):
                data_to_ingest = json.loads(json_data_item)
            else:
                data_to_ingest = json_data_item

            # The actual method to ingest structured JSON needs to be confirmed from graphiti-core docs.
            # The MCP server uses 'add_episode'. The core library might have a similar method.
            # [5] (Graphiti README) mentions quickstart shows adding episodes (text and structured JSON).
            # [23] (MCP Server README) shows: add_episode(name="Doc Name", episode_body=json_string, source="json")
            # We need to map the LlamaParseOutputModel to what graphiti's add_episode (or equivalent) expects.
            # Graphiti's strength is its automatic entity/relationship extraction from text,
            # but for pre-structured JSON, it might directly map fields if Pydantic models are used
            # or if the JSON follows a specific convention Graphiti understands.

            # Option 1: If graphiti can take a Pydantic model instance directly for structured data
            # validated_data = LlamaParseOutputModel(**data_to_ingest)
            # graphiti_instance.add_structured_episode(name=f"doc_{i}", data=validated_data, group_id=group_id) # Hypothetical

            # Option 2: Using add_episode with source="json" if it processes the JSON string
            # This implies graphiti itself will parse the JSON string and extract entities/relationships.
            # If LlamaParse already provides structured entities/relationships, we might need a more direct ingestion.
            # However, the request specifies "graphitti to populate the knowledge graph" from "structured JSON".
            # This implies graphitti has a way to consume this pre-structured JSON.

            episode_name = data_to_ingest.get("title", f"drive_document_{i}")
            # Convert dict back to JSON string if add_episode expects a string
            episode_body_json_str = json.dumps(data_to_ingest)

            # Using the pattern from [23]/[24] for MCP server's add_episode
            # The custom_entities_model parameter in Graphiti is for defining entity types for its *own* extraction
            # from unstructured text. When source="json", Graphiti might have a different way of interpreting it,
            # or it might still try to extract from the JSON values if they are text.
            # This part needs careful alignment with Graphiti's JSON ingestion capabilities.
            # For now, assuming add_episode with source="json" is the primary way.
            graphiti_instance.add_episode(
                name=episode_name,
                episode_body=episode_body_json_str, # Must be a JSON string
                source="json",
                group_id=group_id
                # Potentially, custom_entities_model could be used if graphiti is to further process the JSON content.
                # However, if LlamaParse provides the final structure, we want graphiti to map that structure.
                # This might involve graphiti having specific expectations for the JSON structure when source="json".
            )
            print(f"Successfully ingested document {episode_name} into Graphiti.")
        except Exception as e:
            print(f"Error ingesting document {i} with Graphiti: {e}. Data: {json_data_item}")

    print(f"Completed ingestion of {len(json_outputs)} documents.")


5. The critical aspect here is how graphitti-core interprets the episode_body when source="json". If LlamaParse provides a well-structured JSON with explicit entities and relationships, graphitti should ideally map these directly. If graphitti's source="json" still implies running its own LLM-based extraction on the textual values within the JSON, then the LlamaParse output needs to be shaped accordingly, or a more direct graph-building method using Neo4j drivers might be needed if graphitti lacks direct structured JSON mapping. However, graphitti's documentation suggests it can ingest structured JSON for episode creation.5 The Pydantic models defined here are illustrative; the actual models must align with the LlamaParse output and graphitti's expectations for source="json".



6.4. Orchestration and On-Demand Trigger:

A main Python script (main_ingestion_pipeline.py) will orchestrate these steps:

Call fetch_documents_from_drive().
Pass the results to parse_documents_with_llamaparse(). (This may involve temporary storage of files if GoogleDriveReader gives content and LlamaParse needs paths, or vice-versa).
Pass the LlamaParse JSON outputs to ingest_data_with_graphitti().


The on-demand trigger will initially be a CLI execution of main_ingestion_pipeline.py. For example: python main_ingestion_pipeline.py --drive_folder <folder_id_here_or_use_env>.
Future development will involve wrapping this orchestration logic in a FastAPI endpoint that can be called from the Streamlit frontend.



6.5. Integration and Testing:

Unit Testing: Each script/function (Drive fetch, LlamaParse call, graphitti ingest) should be tested individually with mock data and actual service calls where feasible.
Integration Testing: Test the pipeline end-to-end with a small set of diverse documents (PDFs with text, tables, images; DOCX files, etc.).
Data Validation: After ingestion, query Neo4j directly (e.g., using Neo4j Browser or Cypher queries) to verify that nodes and relationships are created as expected, reflecting the content and structure of the source documents.
RAG System Testing: Perform queries through the Streamlit interface that should leverage the newly ingested knowledge. Compare results before and after ingestion to confirm the new data is being utilized and improving response quality.
Error Handling: Implement robust error handling and logging at each stage of the pipeline to facilitate debugging and provide feedback on failures.


The alignment between LlamaParse's structured JSON output (especially from the "imFeelingLucky" preset or a generic schema) and graphitti's Pydantic model definitions for ingestion is the most technically nuanced part of this implementation. Iterative refinement will likely be necessary based on observed LlamaParse outputs for various document types.7. Conclusion and RecommendationsThe introduction of this on-demand Google Drive data ingestion pipeline represents a significant enhancement to the Graph RAG project. It will empower the system with a more dynamic and expansive knowledge base, directly incorporating user-provided documents into the Neo4j graph. This is expected to lead to more relevant, accurate, and contextually rich responses from the RAG system.To ensure successful implementation and operation, the following recommendations are crucial:

Iterative Testing of LlamaParse Output: The "balanced" parsing strategy with structured JSON output (e.g., using the "imFeelingLucky" preset) from LlamaParse is a cornerstone of this pipeline. It is imperative to conduct thorough testing with a diverse range of real-world documents (PDFs with complex layouts, tables, images, Word documents, etc.) that are expected to be ingested. The resulting JSON structure must be carefully validated for its consistency and suitability for graphitti's ingestion mechanism. Be prepared to refine the approach, which might involve:

Adjusting LlamaParse parameters (e.g., trying the "premium" strategy for particularly complex files if "balanced" is insufficient, though with awareness of potential cost/latency changes).19
Providing more specific parsing instructions or custom JSON schemas to LlamaParse if its default structured output is not optimal.20
Developing more sophisticated Pydantic models and potentially custom ingestion logic within the graphitti script to handle variations in LlamaParse output.
The quality of the graph is directly dependent on the quality of this parsing step.



Resolve .env Discrepancies: The identified inconsistency in GCP_PROJECT_ID (Image 1: neo4j-deployment-new1 vs. Image 2: neo4j-genai-test-project) must be resolved immediately. Confirm the correct Project ID to ensure uninterrupted access to Vertex AI and Google Drive APIs.


Security for Google Drive Access: The Google Drive authentication mechanism, particularly concerning the OAuth 2.0 setup and the role of IMPERSONATED_USER_EMAIL, requires meticulous configuration and review. Ensure that access permissions are strictly limited to the necessary scope (e.g., read-only for the specified folder) and that credentials (credentials.json) are securely managed.17 Misconfiguration could lead to unauthorized data access.


Cost Monitoring: Actively monitor costs associated with the Llama Cloud API (for LlamaParse) and Google Cloud services (Vertex AI, Google Drive API calls). LlamaParse, while offering free tiers, may incur costs for extensive or commercial use.27 Establish budget alerts if possible.


Scalability and Performance Testing: Once the pipeline is functional, conduct tests with a larger volume of documents and concurrent user requests (if the on-demand trigger is exposed via an API) to assess its performance, identify bottlenecks, and ensure it scales appropriately. graphitti's design for real-time incremental updates is advantageous here.22


Robust Error Handling and Logging: Implement comprehensive error handling and detailed logging throughout the pipeline. This is critical for diagnosing issues during on-demand ingestion requests, especially for failures in API calls to Google Drive or LlamaParse, or during graphitti's database operations. Clear logs will be invaluable for troubleshooting.


User Feedback in Streamlit: For the on-demand ingestion feature triggered from Streamlit, provide clear feedback to the user regarding the status of the ingestion process (e.g., "In progress," "Successfully ingested X documents," "Failed: [error message]").


Comprehensive Documentation: Maintain detailed internal documentation covering the setup, configuration, operation, and troubleshooting of this new data ingestion pipeline. This includes Google Drive API setup, LlamaParse API key management, graphitti Pydantic model definitions, and the overall data flow.

By addressing these recommendations, the project team can effectively integrate this powerful new capability, significantly enhancing the Graph RAG system's ability to leverage a broader and more current range of information.
