# Project Structure: kev-new-graph-rag

This document outlines the current structure of the `kev-new-graph-rag` project, with a focus on the main ingestion pipeline script and its components.

## Root Directory
`c:\Users\kevin\repos\kev-new-graph-rag`

## Key Directories & Files

*   **`.env`**: (Not version controlled) Stores environment variables, including API keys (Google, LlamaParse), Neo4j credentials, and other sensitive configuration details.
*   **`config.yml`**: (If present, or general config approach) Defines non-sensitive configuration parameters for various components. Configuration is primarily loaded via `utils.config.Config` which can integrate environment variables and YAML.
*   **`scripts/`**: Contains high-level orchestration scripts.
    *   `ingest_gdrive_documents.py`: The main entry point for the document ingestion pipeline.
*   **`src/`**: Houses the core logic and modules of the application.
    *   **`ontology_templates/`**: Defines the graph schema using Pydantic models.
        *   `generic_ontology.py`: An example ontology defining various node (e.g., `Organization`, `Person`, `Event`) and relationship (e.g., `WorksFor`, `LocatedIn`) types.
    *   **`graph_extraction/`**: Modules responsible for extracting graph data from text content.
        *   `extractor.py`: Contains `GraphExtractor`, which uses the `graphiti-core` library to perform LLM-based graph extraction and ingest into Neo4j.
        *   `gemini_embedder.py`: (Part of Graphiti's internals or a custom wrapper if used directly) Handles text embeddings using Google Gemini models, likely via Graphiti's `BatchSizeOneGeminiEmbedder`.
    *   **`utils/`**: Utility modules providing common functionalities.
        *   `config.py`: Manages loading and accessing application configuration (potentially from `.env` and YAML files).
        *   `config_models.py`: Pydantic models defining the structure for various configuration sections (e.g., `GDriveReaderConfig`, `Neo4jConfig`).
        *   `gdrive_reader.py`: `GDriveReader` class for interacting with Google Drive (listing files, downloading).
        *   `document_parser.py`: `DocumentParser` class, primarily using LlamaParse for parsing document content from various file formats.
        *   `embedding.py`: `CustomGeminiEmbedding` class for generating text embeddings using Google Gemini (used for ChromaDB).
        *   `chroma_ingester.py`: `ChromaIngester` class for ingesting documents and their embeddings into a ChromaDB vector store.
        *   `neo4j_ingester.py`: (Potentially legacy or for direct Neo4j operations if any, as Graphiti now handles its own Neo4j interactions via `GraphExtractor`).
*   **`logs/`**: Directory where log files generated by `loguru` are stored.
*   **`temp/`**: Temporary directory used for storing files downloaded from Google Drive before processing.

## Focus: `scripts/ingest_gdrive_documents.py`

This script orchestrates the entire document ingestion pipeline.

**Purpose:**
To fetch documents from a specified Google Drive folder, parse their content, extract structured graph information (nodes and relationships) based on a defined ontology, and ingest this data into Neo4j. It also ingests document text and embeddings into a ChromaDB vector store.

**Key Imports & Dependencies (Files/Modules it relies on):

*   **Standard Python Libraries:** `argparse`, `nest_asyncio`, `os`, `sys`, `uuid`, `datetime`, `pathlib`, `typing`, `importlib`, `asyncio`.
*   **Third-Party Libraries:**
    *   `dotenv`: For loading environment variables from `.env` files.
    *   `loguru`: For flexible and powerful logging.
    *   `pydantic`: Used for data validation and settings management via Pydantic models (especially for ontology definitions).
*   **Project-Internal Modules:**
    *   `utils.gdrive_reader.GDriveReader` (from `src/utils/gdrive_reader.py`)
    *   `utils.document_parser.DocumentParser` (from `src/utils/document_parser.py`)
    *   `utils.chroma_ingester.ChromaIngester` (from `src/utils/chroma_ingester.py`)
    *   `utils.embedding.CustomGeminiEmbedding` (from `src/utils/embedding.py`)
    *   `utils.config.Config` (from `src/utils/config.py`)
    *   `utils.config_models` (Pydantic models from `src/utils/config_models.py`)
    *   `src.graph_extraction.extractor.GraphExtractor` (from `src/graph_extraction/extractor.py`)
    *   Ontology modules (e.g., `generic_ontology.py`) from `src/ontology_templates/` are dynamically loaded using `importlib` based on the `--template` argument.

**Core Workflow (within `async def main()` and `async def process_documents()`):

1.  **Argument Parsing:** Parses command-line arguments: `--env-file`, `--temp-dir`, `--template` (for ontology selection).
2.  **Configuration Setup (`setup_config` function):**
    *   Loads environment variables (from specified or default `.env` file).
    *   Initializes `Config()` to load other configurations (e.g., from YAML if applicable).
    *   Creates Pydantic configuration objects (e.g., `GDriveReaderConfig`, `LlamaParseConfig`, `Neo4jConfig`, `EmbeddingConfig`).
3.  **Ontology Loading (`load_ontology_from_template` function):**
    *   Dynamically imports the specified ontology module (e.g., `src.ontology_templates.generic_ontology`).
    *   Retrieves `NODES` and `RELATIONSHIPS` lists (Pydantic models) from the loaded module.
4.  **Document Processing (`process_documents` async function):**
    *   Initializes key service components: `GDriveReader`, `DocumentParser`, `ChromaIngester`, `CustomGeminiEmbedding`, and `GraphExtractor`.
    *   Lists files from the configured Google Drive folder.
    *   **For each file:**
        *   Downloads the file to the `temp_dir`.
        *   **LlamaParse:** Parses the document content using `DocumentParser.aparse_file()`.
        *   **ChromaDB Ingestion:**
            *   Creates `Document` objects suitable for ChromaDB.
            *   Generates embeddings for the document content using `CustomGeminiEmbedding`.
            *   Ingests the documents and embeddings into ChromaDB via `ChromaIngester`.
        *   **Graph Extraction & Neo4j Ingestion (via GraphExtractor & Graphiti):**
            *   Retrieves the full text content of the parsed document.
            *   Calls `GraphExtractor.extract()`, passing the text content, and the loaded `ontology_nodes` and `ontology_edges`.
            *   `GraphExtractor` internally uses Graphiti's `add_episode` method, which handles the LLM calls for extraction and the subsequent ingestion of nodes and edges into the connected Neo4j database.
            *   Logs a summary of extracted nodes and edges.
        *   Cleans up the temporary downloaded file.
    *   Logs overall processing statistics (processed, failed, skipped).
    *   Ensures the `GraphExtractor`'s Neo4j connection is closed.
5.  **Output Summary:** Prints a final summary of the ingestion process to the console.
